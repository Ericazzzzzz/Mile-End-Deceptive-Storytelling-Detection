{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 audio files in the dataset.\n",
      "Preview of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Story_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001.wav</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00002.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00003.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004.wav</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00005.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Language       Story_type\n",
       "filename                           \n",
       "00001.wav    Hindi  deceptive_story\n",
       "00002.wav  English       true_story\n",
       "00003.wav  English  deceptive_story\n",
       "00004.wav  Bengali  deceptive_story\n",
       "00005.wav  English  deceptive_story"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the base directory\n",
    "base_path = './MLEnd/deception/MLEndDD_stories_small/'\n",
    "\n",
    "# Load the CSV file and ensure labels are accessible\n",
    "MLEND_df = pd.read_csv('./MLEnd/deception/MLEndDD_story_attributes_small.csv').set_index('filename')\n",
    "\n",
    "# Create a list of full file paths using the CSV index\n",
    "files = [base_path + file for file in MLEND_df.index]\n",
    "\n",
    "# Check the number of files and preview the dataset\n",
    "print(f\"We have {len(files)} audio files in the dataset.\")\n",
    "print(\"Preview of the dataset:\")\n",
    "display(MLEND_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Audio Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Duration (s)</th>\n",
       "      <th>Number of Chunks</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>122.167256</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>125.192018</td>\n",
       "      <td>4</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.wav</td>\n",
       "      <td>162.984127</td>\n",
       "      <td>5</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.wav</td>\n",
       "      <td>121.681270</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.wav</td>\n",
       "      <td>134.189751</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>00096.wav</td>\n",
       "      <td>111.512063</td>\n",
       "      <td>3</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>00097.wav</td>\n",
       "      <td>185.731224</td>\n",
       "      <td>6</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>00098.wav</td>\n",
       "      <td>128.252766</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>00099.wav</td>\n",
       "      <td>132.412562</td>\n",
       "      <td>4</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>00100.wav</td>\n",
       "      <td>123.273560</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      File ID  Duration (s)  Number of Chunks            Label\n",
       "0   00001.wav    122.167256                 4  deceptive_story\n",
       "1   00002.wav    125.192018                 4       true_story\n",
       "2   00003.wav    162.984127                 5  deceptive_story\n",
       "3   00004.wav    121.681270                 4  deceptive_story\n",
       "4   00005.wav    134.189751                 4  deceptive_story\n",
       "..        ...           ...               ...              ...\n",
       "95  00096.wav    111.512063                 3  deceptive_story\n",
       "96  00097.wav    185.731224                 6       true_story\n",
       "97  00098.wav    128.252766                 4  deceptive_story\n",
       "98  00099.wav    132.412562                 4       true_story\n",
       "99  00100.wav    123.273560                 4  deceptive_story\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Audio Chunks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>File ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav_chunk1</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001.wav_chunk2</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.wav_chunk3</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001.wav_chunk4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00002.wav_chunk1</td>\n",
       "      <td>true_story</td>\n",
       "      <td>00002.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>00099.wav_chunk4</td>\n",
       "      <td>true_story</td>\n",
       "      <td>00099.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>00100.wav_chunk1</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>00100.wav_chunk2</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>00100.wav_chunk3</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>00100.wav_chunk4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Chunk ID            Label    File ID\n",
       "0    00001.wav_chunk1  deceptive_story  00001.wav\n",
       "1    00001.wav_chunk2  deceptive_story  00001.wav\n",
       "2    00001.wav_chunk3  deceptive_story  00001.wav\n",
       "3    00001.wav_chunk4  deceptive_story  00001.wav\n",
       "4    00002.wav_chunk1       true_story  00002.wav\n",
       "..                ...              ...        ...\n",
       "415  00099.wav_chunk4       true_story  00099.wav\n",
       "416  00100.wav_chunk1  deceptive_story  00100.wav\n",
       "417  00100.wav_chunk2  deceptive_story  00100.wav\n",
       "418  00100.wav_chunk3  deceptive_story  00100.wav\n",
       "419  00100.wav_chunk4  deceptive_story  00100.wav\n",
       "\n",
       "[420 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid audio chunks created: 420\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_audio_into_chunks(file_id, file_path, label, chunk_duration=30, sr=None):\n",
    "    \"\"\"\n",
    "    Splits an audio file into 30-second chunks and discards chunks shorter than 30 seconds.\n",
    "\n",
    "    Args:\n",
    "        file_id (str): The file ID (original file name).\n",
    "        file_path (str): Path to the audio file.\n",
    "        label (str): Label for the audio file (e.g., 'true_story' or 'deceptive_story').\n",
    "        chunk_duration (int): Duration of each chunk in seconds (default: 30).\n",
    "        sr (int or None): Sampling rate. If None, uses the original rate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: File metadata (duration, number of valid chunks, label) and a list of chunk data with IDs.\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=sr)  # Load audio\n",
    "    duration = len(y) / sr  # Calculate total duration in seconds\n",
    "    chunk_size = chunk_duration * sr       # Calculate chunk size in samples\n",
    "    chunks = [y[i:i + int(chunk_size)] for i in range(0, len(y), int(chunk_size))]\n",
    "\n",
    "    # Discard chunks shorter than the desired length\n",
    "    valid_chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "\n",
    "    # Prepare metadata and chunks with Chunk ID\n",
    "    metadata = {\"File ID\": file_id, \"Duration (s)\": duration, \"Number of Chunks\": len(valid_chunks), \"Label\": label}\n",
    "    chunk_data = [(f\"{file_id}_chunk{i+1}\", chunk, label, file_id) for i, chunk in enumerate(valid_chunks)]\n",
    "    \n",
    "    return metadata, chunk_data\n",
    "\n",
    "# Initialize lists for metadata and chunks\n",
    "file_metadata = []\n",
    "audio_chunks = []\n",
    "\n",
    "# Iterate over all files and process\n",
    "for file_id in tqdm(MLEND_df.index):  # Iterate over file IDs in the CSV\n",
    "    file_path = base_path + file_id   # Construct full file path\n",
    "    label = MLEND_df.loc[file_id, 'Story_type']  # Retrieve label from the CSV\n",
    "\n",
    "    # Split into chunks and collect metadata\n",
    "    metadata, chunks = split_audio_into_chunks(file_id, file_path, label)\n",
    "    file_metadata.append(metadata)  # Collect metadata for the file\n",
    "    audio_chunks.extend(chunks)     # Collect valid chunks\n",
    "\n",
    "# Convert metadata to a DataFrame for easy viewing\n",
    "metadata_df = pd.DataFrame(file_metadata)\n",
    "\n",
    "# Display metadata\n",
    "print(\"Summary of Audio Files:\")\n",
    "display(metadata_df)\n",
    "\n",
    "# Prepare a DataFrame for audio chunks\n",
    "chunks_df = pd.DataFrame(audio_chunks, columns=[\"Chunk ID\", \"Chunk Data\", \"Label\", \"File ID\"])\n",
    "\n",
    "# Display chunk summary\n",
    "print(\"Summary of Audio Chunks:\")\n",
    "display(chunks_df[[\"Chunk ID\", \"Label\", \"File ID\"]])\n",
    "\n",
    "# Output the total number of valid chunks\n",
    "print(f\"Total valid audio chunks created: {len(audio_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Chunk Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>File ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav_chunk1</td>\n",
       "      <td>[1.5258789e-05, 1.5258789e-05, 3.0517578e-05, ...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001.wav_chunk2</td>\n",
       "      <td>[0.027450562, 0.026519775, 0.025390625, 0.0242...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.wav_chunk3</td>\n",
       "      <td>[-0.00091552734, -0.0011138916, -0.0013122559,...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001.wav_chunk4</td>\n",
       "      <td>[6.1035156e-05, 9.1552734e-05, 7.6293945e-05, ...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00001.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00002.wav_chunk1</td>\n",
       "      <td>[0.0008239746, 0.0008239746, 0.00088500977, 0....</td>\n",
       "      <td>true_story</td>\n",
       "      <td>00002.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>00099.wav_chunk4</td>\n",
       "      <td>[-3.0517578e-05, -3.0517578e-05, -3.0517578e-0...</td>\n",
       "      <td>true_story</td>\n",
       "      <td>00099.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>00100.wav_chunk1</td>\n",
       "      <td>[-0.00018310547, -0.00015258789, -6.1035156e-0...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>00100.wav_chunk2</td>\n",
       "      <td>[0.0004272461, 0.00048828125, 0.0005187988, 0....</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>00100.wav_chunk3</td>\n",
       "      <td>[6.1035156e-05, 0.0, -6.1035156e-05, -6.103515...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>00100.wav_chunk4</td>\n",
       "      <td>[3.0517578e-05, 0.0, 3.0517578e-05, 0.00012207...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>00100.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Chunk ID                                         Chunk Data  \\\n",
       "0    00001.wav_chunk1  [1.5258789e-05, 1.5258789e-05, 3.0517578e-05, ...   \n",
       "1    00001.wav_chunk2  [0.027450562, 0.026519775, 0.025390625, 0.0242...   \n",
       "2    00001.wav_chunk3  [-0.00091552734, -0.0011138916, -0.0013122559,...   \n",
       "3    00001.wav_chunk4  [6.1035156e-05, 9.1552734e-05, 7.6293945e-05, ...   \n",
       "4    00002.wav_chunk1  [0.0008239746, 0.0008239746, 0.00088500977, 0....   \n",
       "..                ...                                                ...   \n",
       "415  00099.wav_chunk4  [-3.0517578e-05, -3.0517578e-05, -3.0517578e-0...   \n",
       "416  00100.wav_chunk1  [-0.00018310547, -0.00015258789, -6.1035156e-0...   \n",
       "417  00100.wav_chunk2  [0.0004272461, 0.00048828125, 0.0005187988, 0....   \n",
       "418  00100.wav_chunk3  [6.1035156e-05, 0.0, -6.1035156e-05, -6.103515...   \n",
       "419  00100.wav_chunk4  [3.0517578e-05, 0.0, 3.0517578e-05, 0.00012207...   \n",
       "\n",
       "               Label    File ID  \n",
       "0    deceptive_story  00001.wav  \n",
       "1    deceptive_story  00001.wav  \n",
       "2    deceptive_story  00001.wav  \n",
       "3    deceptive_story  00001.wav  \n",
       "4         true_story  00002.wav  \n",
       "..               ...        ...  \n",
       "415       true_story  00099.wav  \n",
       "416  deceptive_story  00100.wav  \n",
       "417  deceptive_story  00100.wav  \n",
       "418  deceptive_story  00100.wav  \n",
       "419  deceptive_story  00100.wav  \n",
       "\n",
       "[420 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420/420 [12:26<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (420, 17)\n",
      "Label vector shape: (420,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_features_and_labels(chunks_df, n_mfcc=13, sr=16000, scale_audio=True):\n",
    "    \"\"\"\n",
    "    Extract features (MFCC, Pitch, Energy, ZCR) and associate labels for 30-second audio chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks_df (DataFrame): DataFrame containing audio chunks and metadata.\n",
    "        n_mfcc (int): Number of MFCC coefficients to extract.\n",
    "        sr (int): Sampling rate for feature extraction.\n",
    "        scale_audio (bool): Whether to scale audio amplitude.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix (X) with extracted features for each chunk.\n",
    "        np.ndarray: Label vector (y) corresponding to each chunk.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in tqdm(chunks_df.iterrows(), total=len(chunks_df)):\n",
    "        chunk = row[\"Chunk Data\"]\n",
    "        label = row[\"Label\"]\n",
    "\n",
    "        # Scale the audio if required\n",
    "        if scale_audio:\n",
    "            chunk = chunk / np.max(np.abs(chunk)) if np.max(np.abs(chunk)) > 0 else chunk\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc=n_mfcc).mean(axis=1)\n",
    "\n",
    "        # Extract Pitch features\n",
    "        pitch, _, _ = librosa.pyin(chunk, fmin=80, fmax=450, sr=sr)\n",
    "        pitch_mean = np.nanmean(pitch) if np.mean(np.isnan(pitch)) < 1 else 0\n",
    "        pitch_std = np.nanstd(pitch) if np.mean(np.isnan(pitch)) < 1 else 0\n",
    "\n",
    "        # Compute Energy (RMS)\n",
    "        rms = np.mean(librosa.feature.rms(y=chunk))\n",
    "\n",
    "        # Compute Zero-Crossing Rate (ZCR)\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y=chunk))\n",
    "\n",
    "        # Combine all features into a single feature vector\n",
    "        xi = np.concatenate([mfcc, [pitch_mean, pitch_std, rms, zcr]])\n",
    "        X.append(xi)\n",
    "\n",
    "        # Append the corresponding label\n",
    "        y.append(1 if label == 'true_story' else 0)  # Binary encode labels\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Perform feature extraction\n",
    "X, y = extract_features_and_labels(chunks_df, n_mfcc=13, sr=16000, scale_audio=True)\n",
    "\n",
    "# Print the shapes of the feature matrix and label vector\n",
    "print(f\"Feature matrix shape: {X.shape}\")  # Rows: chunks, Columns: features\n",
    "print(f\"Label vector shape: {y.shape}\")  # Labels for each chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (252, 17), Labels: (252,)\n",
      "Validation set size: (84, 17), Labels: (84,)\n",
      "Test set size: (84, 17), Labels: (84,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform data splitting\n",
    "def split_data(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Label vector.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Split datasets (X_train, X_val, X_test, y_train, y_val, y_test).\n",
    "    \"\"\"\n",
    "    # Split the data into training+validation (80%) and test (20%) sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Further split training+validation into train (75%) and validation (25%) sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Perform the split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "# Output the shapes of the splits\n",
    "print(f\"Training set size: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}, Labels: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in Training set:\n",
      "  0: 131 (51.98%)\n",
      "  1: 121 (48.02%)\n",
      "\n",
      "Label distribution in Validation set:\n",
      "  0: 44 (52.38%)\n",
      "  1: 40 (47.62%)\n",
      "\n",
      "Label distribution in Test set:\n",
      "  0: 44 (52.38%)\n",
      "  1: 40 (47.62%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check_label_distribution(labels, split_name):\n",
    "    \"\"\"\n",
    "    Checks and prints the distribution of labels.\n",
    "\n",
    "    Args:\n",
    "        labels (np.ndarray): Array of labels.\n",
    "        split_name (str): Name of the dataset split (e.g., \"Training\").\n",
    "    \"\"\"\n",
    "    label_counts = Counter(labels)\n",
    "    total = sum(label_counts.values())\n",
    "    print(f\"\\nLabel distribution in {split_name} set:\")\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# Check distributions\n",
    "check_label_distribution(y_train, \"Training\")\n",
    "check_label_distribution(y_val, \"Validation\")\n",
    "check_label_distribution(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_individual_features_and_labels(audio_chunks, sr=22050, scale_audio=True):\n",
    "    \"\"\"\n",
    "    Extract individual features (MFCC, Pitch, Energy, ZCR) and associate labels for 30-second audio chunks.\n",
    "\n",
    "    Args:\n",
    "        audio_chunks (list): List of tuples [(chunk_id, chunk, label, file_id)].\n",
    "        sr (int): Sampling rate for feature extraction.\n",
    "        scale_audio (bool): Whether to scale audio amplitude.\n",
    "\n",
    "    Returns:\n",
    "        dict: Feature matrices for each feature type (MFCC, Pitch, Energy, ZCR).\n",
    "        np.ndarray: Label vector corresponding to each chunk.\n",
    "    \"\"\"\n",
    "    mfcc_features, pitch_features, energy_features, zcr_features = [], [], [], []\n",
    "    labels = []\n",
    "\n",
    "    for chunk_id, chunk, label, file_id in tqdm(audio_chunks):\n",
    "        # Scale the audio if required\n",
    "        if scale_audio:\n",
    "            chunk = chunk / np.max(np.abs(chunk)) if np.max(np.abs(chunk)) > 0 else chunk\n",
    "\n",
    "        # Extract MFCC features\n",
    "        n_mfcc = 13\n",
    "        mfcc = librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc=n_mfcc).mean(axis=1)\n",
    "\n",
    "        # Extract Pitch features\n",
    "        try:\n",
    "            pitch, _, _ = librosa.pyin(chunk, fmin=80, fmax=450, sr=sr)\n",
    "            pitch_mean = np.nanmean(pitch) if np.mean(np.isnan(pitch)) < 1 else 0\n",
    "            pitch_std = np.nanstd(pitch) if np.mean(np.isnan(pitch)) < 1 else 0\n",
    "        except librosa.util.exceptions.ParameterError:\n",
    "            pitch_mean, pitch_std = 0, 0\n",
    "\n",
    "        # Compute Energy (RMS)\n",
    "        rms = np.mean(librosa.feature.rms(y=chunk))\n",
    "\n",
    "        # Compute Zero-Crossing Rate (ZCR)\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y=chunk))\n",
    "\n",
    "        # Append individual feature vectors\n",
    "        mfcc_features.append(mfcc)\n",
    "        pitch_features.append([pitch_mean, pitch_std])\n",
    "        energy_features.append([rms])\n",
    "        zcr_features.append([zcr])\n",
    "\n",
    "        # Append the corresponding label\n",
    "        labels.append(1 if label == 'deceptive_story' else 0)  # Binary encode labels\n",
    "\n",
    "    # Convert features and labels to numpy arrays\n",
    "    return {\n",
    "        'MFCC': np.array(mfcc_features),\n",
    "        'Pitch': np.array(pitch_features),\n",
    "        'Energy': np.array(energy_features),\n",
    "        'ZCR': np.array(zcr_features)\n",
    "    }, np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420/420 [12:02<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC Feature Matrix Shape: (420, 13)\n",
      "Pitch Feature Matrix Shape: (420, 2)\n",
      "Energy Feature Matrix Shape: (420, 1)\n",
      "ZCR Feature Matrix Shape: (420, 1)\n",
      "Label Vector Shape: (420,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels separately for each feature type\n",
    "features, y = extract_individual_features_and_labels(audio_chunks, sr=22050)\n",
    "\n",
    "# Unpack individual feature matrices\n",
    "X_mfcc = features['MFCC']\n",
    "X_pitch = features['Pitch']\n",
    "X_energy = features['Energy']\n",
    "X_zcr = features['ZCR']\n",
    "\n",
    "# Display shapes for debugging\n",
    "print(f\"MFCC Feature Matrix Shape: {X_mfcc.shape}\")\n",
    "print(f\"Pitch Feature Matrix Shape: {X_pitch.shape}\")\n",
    "print(f\"Energy Feature Matrix Shape: {X_energy.shape}\")\n",
    "print(f\"ZCR Feature Matrix Shape: {X_zcr.shape}\")\n",
    "print(f\"Label Vector Shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_nested_train_valid_test(X, y, test_size=0.2, valid_ratio=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits data into train-validation and test sets, then further splits train-validation into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Labels.\n",
    "        test_size (float): Proportion of data to be used as test set.\n",
    "        valid_ratio (float): Proportion of the train-validation set to be used for validation.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Train, validation, and test sets with normalized features and labels.\n",
    "    \"\"\"\n",
    "    # Split into train-validation and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Split train-validation into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=valid_ratio, stratify=y_train_val, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC Train Shape: (252, 13), Validation Shape: (84, 13), Test Shape: (84, 13)\n",
      "Pitch Train Shape: (252, 2), Validation Shape: (84, 2), Test Shape: (84, 2)\n",
      "Energy Train Shape: (252, 1), Validation Shape: (84, 1), Test Shape: (84, 1)\n",
      "ZCR Train Shape: (252, 1), Validation Shape: (84, 1), Test Shape: (84, 1)\n",
      "Labels Train: 252, Validation: 84, Test: 84\n"
     ]
    }
   ],
   "source": [
    "# Split each feature type into train-validation and test sets\n",
    "X_mfcc_train, X_mfcc_val, X_mfcc_test, y_train, y_val, y_test = prepare_nested_train_valid_test(X_mfcc, y)\n",
    "X_pitch_train, X_pitch_val, X_pitch_test, _, _, _ = prepare_nested_train_valid_test(X_pitch, y)\n",
    "X_energy_train, X_energy_val, X_energy_test, _, _, _ = prepare_nested_train_valid_test(X_energy, y)\n",
    "X_zcr_train, X_zcr_val, X_zcr_test, _, _, _ = prepare_nested_train_valid_test(X_zcr, y)\n",
    "\n",
    "# Confirm the sizes of the splits\n",
    "print(f\"MFCC Train Shape: {X_mfcc_train.shape}, Validation Shape: {X_mfcc_val.shape}, Test Shape: {X_mfcc_test.shape}\")\n",
    "print(f\"Pitch Train Shape: {X_pitch_train.shape}, Validation Shape: {X_pitch_val.shape}, Test Shape: {X_pitch_test.shape}\")\n",
    "print(f\"Energy Train Shape: {X_energy_train.shape}, Validation Shape: {X_energy_val.shape}, Test Shape: {X_energy_test.shape}\")\n",
    "print(f\"ZCR Train Shape: {X_zcr_train.shape}, Validation Shape: {X_zcr_val.shape}, Test Shape: {X_zcr_test.shape}\")\n",
    "print(f\"Labels Train: {len(y_train)}, Validation: {len(y_val)}, Test: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance on MFCC Features (Validation Set):\n",
      "  Logistic Regression: F1 Score = 0.46, Accuracy = 0.52\n",
      "  Decision Tree: F1 Score = 0.68, Accuracy = 0.70\n",
      "  SVM: F1 Score = 0.57, Accuracy = 0.61\n",
      "\n",
      "Performance on Pitch Features (Validation Set):\n",
      "  Logistic Regression: F1 Score = 0.12, Accuracy = 0.50\n",
      "  Decision Tree: F1 Score = 0.55, Accuracy = 0.56\n",
      "  SVM: F1 Score = 0.14, Accuracy = 0.55\n",
      "\n",
      "Performance on Energy Features (Validation Set):\n",
      "  Logistic Regression: F1 Score = 0.00, Accuracy = 0.52\n",
      "  Decision Tree: F1 Score = 0.47, Accuracy = 0.46\n",
      "  SVM: F1 Score = 0.00, Accuracy = 0.52\n",
      "\n",
      "Performance on ZCR Features (Validation Set):\n",
      "  Logistic Regression: F1 Score = 0.26, Accuracy = 0.54\n",
      "  Decision Tree: F1 Score = 0.53, Accuracy = 0.58\n",
      "  SVM: F1 Score = 0.05, Accuracy = 0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Function to train and evaluate models on validation set\n",
    "def train_and_evaluate_on_validation(feature_name, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using validation set.\n",
    "\n",
    "    Args:\n",
    "        feature_name (str): Name of the feature (e.g., 'MFCC').\n",
    "        X_train (np.ndarray): Training feature set.\n",
    "        X_val (np.ndarray): Validation feature set.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Validation F1 score and accuracy for each model.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'SVM': SVC(kernel='linear', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        y_pred = model.predict(X_val)  # Predict on validation set\n",
    "        f1 = f1_score(y_val, y_pred)  # F1 score\n",
    "        acc = accuracy_score(y_val, y_pred)  # Accuracy\n",
    "        results[name] = {'F1 Score': f1, 'Accuracy': acc}\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nPerformance on {feature_name} Features (Validation Set):\")\n",
    "    for model, metrics in results.items():\n",
    "        print(f\"  {model}: F1 Score = {metrics['F1 Score']:.2f}, Accuracy = {metrics['Accuracy']:.2f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Train and evaluate models on each feature type\n",
    "mfcc_results = train_and_evaluate_on_validation('MFCC', X_mfcc_train, X_mfcc_val, y_train, y_val)\n",
    "pitch_results = train_and_evaluate_on_validation('Pitch', X_pitch_train, X_pitch_val, y_train, y_val)\n",
    "energy_results = train_and_evaluate_on_validation('Energy', X_energy_train, X_energy_val, y_train, y_val)\n",
    "zcr_results = train_and_evaluate_on_validation('ZCR', X_zcr_train, X_zcr_val, y_train, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models on each feature type in the specified order\n",
    "mfcc_results = train_and_evaluate_on_validation('MFCC', X_mfcc_train, X_mfcc_val, y_train, y_val)\n",
    "pitch_results = train_and_evaluate_on_validation('Pitch', X_pitch_train, X_pitch_val, y_train, y_val)\n",
    "energy_results = train_and_evaluate_on_validation('Energy', X_energy_train, X_energy_val, y_train, y_val)\n",
    "zcr_results = train_and_evaluate_on_validation('ZCR', X_zcr_train, X_zcr_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def train_and_evaluate_svm(feature_name, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM using training and validation sets for both linear and RBF kernels.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # SVM with Linear Kernel\n",
    "    linear_model = SVC(kernel='linear', random_state=42)\n",
    "    linear_model.fit(X_train, y_train)  # Train the model\n",
    "    y_train_pred_linear = linear_model.predict(X_train)\n",
    "    y_val_pred_linear = linear_model.predict(X_val)\n",
    "\n",
    "    # Linear Kernel Metrics\n",
    "    train_acc_linear = accuracy_score(y_train, y_train_pred_linear)\n",
    "    val_acc_linear = accuracy_score(y_val, y_val_pred_linear)\n",
    "    train_f1_linear = f1_score(y_train, y_train_pred_linear)\n",
    "    val_f1_linear = f1_score(y_val, y_val_pred_linear)\n",
    "\n",
    "    results['Linear Kernel'] = {\n",
    "        'Training Accuracy': train_acc_linear,\n",
    "        'Validation Accuracy': val_acc_linear,\n",
    "        'Training F1 Score': train_f1_linear,\n",
    "        'Validation F1 Score': val_f1_linear\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSVM (Linear Kernel) Performance on {feature_name} Features:\")\n",
    "    print(f\"  Accuracy -> Training: {train_acc_linear:.2f}, Validation: {val_acc_linear:.2f}\")\n",
    "    print(f\"  F1 Score -> Training: {train_f1_linear:.2f}, Validation: {val_f1_linear:.2f}\")\n",
    "\n",
    "    # SVM with RBF Kernel\n",
    "    rbf_model = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "    rbf_model.fit(X_train, y_train)  # Train the model\n",
    "    y_train_pred_rbf = rbf_model.predict(X_train)\n",
    "    y_val_pred_rbf = rbf_model.predict(X_val)\n",
    "\n",
    "    # RBF Kernel Metrics\n",
    "    train_acc_rbf = accuracy_score(y_train, y_train_pred_rbf)\n",
    "    val_acc_rbf = accuracy_score(y_val, y_val_pred_rbf)\n",
    "    train_f1_rbf = f1_score(y_train, y_train_pred_rbf)\n",
    "    val_f1_rbf = f1_score(y_val, y_val_pred_rbf)\n",
    "\n",
    "    results['RBF Kernel'] = {\n",
    "        'Training Accuracy': train_acc_rbf,\n",
    "        'Validation Accuracy': val_acc_rbf,\n",
    "        'Training F1 Score': train_f1_rbf,\n",
    "        'Validation F1 Score': val_f1_rbf\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSVM (RBF Kernel, C=1, Gamma='scale') Performance on {feature_name} Features:\")\n",
    "    print(f\"  Accuracy -> Training: {train_acc_rbf:.2f}, Validation: {val_acc_rbf:.2f}\")\n",
    "    print(f\"  F1 Score -> Training: {train_f1_rbf:.2f}, Validation: {val_f1_rbf:.2f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def train_and_evaluate_logistic_regression(feature_name, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate Logistic Regression using training and validation sets.\n",
    "    \"\"\"\n",
    "    model = LogisticRegression(max_iter=1000,random_state=42)\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    \n",
    "    # Predictions for train and validation sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # F1 scores\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"\\nLogistic Regression Performance on {feature_name} Features:\")\n",
    "    print(f\"  Accuracy -> Training: {train_acc:.2f}, Validation: {val_acc:.2f}\")\n",
    "    print(f\"  F1 Score -> Training: {train_f1:.2f}, Validation: {val_f1:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'Training Accuracy': train_acc, \n",
    "        'Validation Accuracy': val_acc,\n",
    "        'Training F1 Score': train_f1, \n",
    "        'Validation F1 Score': val_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_and_evaluate_decision_tree(feature_name, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate Decision Tree using training and validation sets.\n",
    "    \"\"\"\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    \n",
    "    # Predictions for train and validation sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # F1 scores\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"\\nDecision Tree Performance on {feature_name} Features:\")\n",
    "    print(f\"  Accuracy -> Training: {train_acc:.2f}, Validation: {val_acc:.2f}\")\n",
    "    print(f\"  F1 Score -> Training: {train_f1:.2f}, Validation: {val_f1:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'Training Accuracy': train_acc, \n",
    "        'Validation Accuracy': val_acc,\n",
    "        'Training F1 Score': train_f1, \n",
    "        'Validation F1 Score': val_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SVM...\n",
      "\n",
      "SVM (Linear Kernel) Performance on MFCC Features:\n",
      "  Accuracy -> Training: 0.66, Validation: 0.61\n",
      "  F1 Score -> Training: 0.65, Validation: 0.57\n",
      "\n",
      "SVM (RBF Kernel, C=1, Gamma='scale') Performance on MFCC Features:\n",
      "  Accuracy -> Training: 0.94, Validation: 0.73\n",
      "  F1 Score -> Training: 0.93, Validation: 0.69\n",
      "\n",
      "SVM (Linear Kernel) Performance on Pitch Features:\n",
      "  Accuracy -> Training: 0.53, Validation: 0.55\n",
      "  F1 Score -> Training: 0.12, Validation: 0.14\n",
      "\n",
      "SVM (RBF Kernel, C=1, Gamma='scale') Performance on Pitch Features:\n",
      "  Accuracy -> Training: 0.66, Validation: 0.40\n",
      "  F1 Score -> Training: 0.57, Validation: 0.17\n",
      "\n",
      "SVM (Linear Kernel) Performance on Energy Features:\n",
      "  Accuracy -> Training: 0.52, Validation: 0.52\n",
      "  F1 Score -> Training: 0.00, Validation: 0.00\n",
      "\n",
      "SVM (RBF Kernel, C=1, Gamma='scale') Performance on Energy Features:\n",
      "  Accuracy -> Training: 0.55, Validation: 0.51\n",
      "  F1 Score -> Training: 0.16, Validation: 0.05\n",
      "\n",
      "SVM (Linear Kernel) Performance on ZCR Features:\n",
      "  Accuracy -> Training: 0.52, Validation: 0.54\n",
      "  F1 Score -> Training: 0.05, Validation: 0.05\n",
      "\n",
      "SVM (RBF Kernel, C=1, Gamma='scale') Performance on ZCR Features:\n",
      "  Accuracy -> Training: 0.58, Validation: 0.62\n",
      "  F1 Score -> Training: 0.41, Validation: 0.43\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "\n",
      "Logistic Regression Performance on MFCC Features:\n",
      "  Accuracy -> Training: 0.64, Validation: 0.52\n",
      "  F1 Score -> Training: 0.62, Validation: 0.46\n",
      "\n",
      "Logistic Regression Performance on Pitch Features:\n",
      "  Accuracy -> Training: 0.56, Validation: 0.50\n",
      "  F1 Score -> Training: 0.29, Validation: 0.12\n",
      "\n",
      "Logistic Regression Performance on Energy Features:\n",
      "  Accuracy -> Training: 0.52, Validation: 0.52\n",
      "  F1 Score -> Training: 0.02, Validation: 0.00\n",
      "\n",
      "Logistic Regression Performance on ZCR Features:\n",
      "  Accuracy -> Training: 0.55, Validation: 0.54\n",
      "  F1 Score -> Training: 0.37, Validation: 0.26\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "\n",
      "Decision Tree Performance on MFCC Features:\n",
      "  Accuracy -> Training: 1.00, Validation: 0.70\n",
      "  F1 Score -> Training: 1.00, Validation: 0.68\n",
      "\n",
      "Decision Tree Performance on Pitch Features:\n",
      "  Accuracy -> Training: 1.00, Validation: 0.56\n",
      "  F1 Score -> Training: 1.00, Validation: 0.55\n",
      "\n",
      "Decision Tree Performance on Energy Features:\n",
      "  Accuracy -> Training: 1.00, Validation: 0.46\n",
      "  F1 Score -> Training: 1.00, Validation: 0.47\n",
      "\n",
      "Decision Tree Performance on ZCR Features:\n",
      "  Accuracy -> Training: 1.00, Validation: 0.58\n",
      "  F1 Score -> Training: 1.00, Validation: 0.53\n"
     ]
    }
   ],
   "source": [
    "# Evaluate SVM for each feature\n",
    "print(\"\\nEvaluating SVM...\")\n",
    "mfcc_svm_results = train_and_evaluate_svm('MFCC', X_mfcc_train, X_mfcc_val, y_train, y_val)\n",
    "pitch_svm_results = train_and_evaluate_svm('Pitch', X_pitch_train, X_pitch_val, y_train, y_val)\n",
    "energy_svm_results = train_and_evaluate_svm('Energy', X_energy_train, X_energy_val, y_train, y_val)\n",
    "zcr_svm_results = train_and_evaluate_svm('ZCR', X_zcr_train, X_zcr_val, y_train, y_val)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\nEvaluating Logistic Regression...\")\n",
    "mfcc_lr_results = train_and_evaluate_logistic_regression('MFCC', X_mfcc_train, X_mfcc_val, y_train, y_val)\n",
    "pitch_lr_results = train_and_evaluate_logistic_regression('Pitch', X_pitch_train, X_pitch_val, y_train, y_val)\n",
    "energy_lr_results = train_and_evaluate_logistic_regression('Energy', X_energy_train, X_energy_val, y_train, y_val)\n",
    "zcr_lr_results = train_and_evaluate_logistic_regression('ZCR', X_zcr_train, X_zcr_val, y_train, y_val)\n",
    "\n",
    "# Decision Tree\n",
    "print(\"\\nEvaluating Decision Tree...\")\n",
    "mfcc_dt_results = train_and_evaluate_decision_tree('MFCC', X_mfcc_train, X_mfcc_val, y_train, y_val)\n",
    "pitch_dt_results = train_and_evaluate_decision_tree('Pitch', X_pitch_train, X_pitch_val, y_train, y_val)\n",
    "energy_dt_results = train_and_evaluate_decision_tree('Energy', X_energy_train, X_energy_val, y_train, y_val)\n",
    "zcr_dt_results = train_and_evaluate_decision_tree('ZCR', X_zcr_train, X_zcr_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Collect results for all features\n",
    "# all_results = {\n",
    "#     'MFCC': mfcc_results,\n",
    "#     'Pitch': pitch_results,\n",
    "#     'Energy': energy_results,\n",
    "#     'ZCR': zcr_results\n",
    "# }\n",
    "\n",
    "# # Prepare data for a multi-level column structure\n",
    "# data = []\n",
    "# models = list(all_results['MFCC'].keys())  # Assume all feature results have the same models\n",
    "\n",
    "# for model in models:\n",
    "#     row = []\n",
    "#     for feature, results in all_results.items():\n",
    "#         metrics = results[model]\n",
    "#         row.extend([metrics['Training Accuracy'], metrics['Validation Accuracy']])\n",
    "#     data.append(row)\n",
    "\n",
    "# # Define multi-level column names\n",
    "# columns = pd.MultiIndex.from_product(\n",
    "#     [all_results.keys(), ['Training Accuracy', 'Validation Accuracy']],\n",
    "#     names=['Feature', 'Metric']\n",
    "# )\n",
    "\n",
    "# # Create the DataFrame\n",
    "# summary_df = pd.DataFrame(data, index=models, columns=columns)\n",
    "\n",
    "# # Display the summary table\n",
    "# print(\"\\nTraining and Validation Results Across Features:\")\n",
    "# display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare data for F1 scores\n",
    "# f1_data = []\n",
    "\n",
    "# for model in models:  # Use the same models list\n",
    "#     row = []\n",
    "#     for feature, results in all_results.items():\n",
    "#         metrics = results[model]\n",
    "#         row.extend([metrics['Training F1 Score'], metrics['Validation F1 Score']])\n",
    "#     f1_data.append(row)\n",
    "\n",
    "# # Define multi-level column names for F1 Scores\n",
    "# f1_columns = pd.MultiIndex.from_product(\n",
    "#     [all_results.keys(), ['Training F1 Score', 'Validation F1 Score']],\n",
    "#     names=['Feature', 'Metric']\n",
    "# )\n",
    "\n",
    "# # Create the DataFrame for F1 Scores\n",
    "# f1_summary_df = pd.DataFrame(f1_data, index=models, columns=f1_columns)\n",
    "\n",
    "# # Display the F1 score summary table\n",
    "# print(\"\\nTraining and Validation F1 Scores Across Features:\")\n",
    "# display(f1_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Training and Validation Metrics (Accuracy and F1 Scores) Across Features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th colspan=\"4\" halign=\"left\">MFCC</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Pitch</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Energy</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ZCR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1 Score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1 Score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1 Score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">F1 Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Set</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Training</th>\n",
       "      <th>Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM (Linear Kernel)</th>\n",
       "      <td>0.658730</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.653226</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.519841</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519841</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.048780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM (RBF Kernel)</th>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.662698</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.408840</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.619247</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.264151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.817460</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>0.673684</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.616034</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.626984</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.507463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Feature                  MFCC                                     Pitch  \\\n",
       "Metric               Accuracy             F1 Score             Accuracy   \n",
       "Set                  Training Validation  Training Validation  Training   \n",
       "SVM (Linear Kernel)  0.658730   0.607143  0.653226   0.571429  0.527778   \n",
       "SVM (RBF Kernel)     0.936508   0.726190  0.933333   0.693333  0.662698   \n",
       "Logistic Regression  0.638889   0.523810  0.619247   0.459459  0.563492   \n",
       "Decision Tree        0.817460   0.630952  0.839161   0.673684  0.742063   \n",
       "\n",
       "Feature                                                Energy             \\\n",
       "Metric                          F1 Score             Accuracy              \n",
       "Set                 Validation  Training Validation  Training Validation   \n",
       "SVM (Linear Kernel)   0.547619  0.118519   0.136364  0.519841   0.523810   \n",
       "SVM (RBF Kernel)      0.404762  0.572864   0.166667  0.547619   0.511905   \n",
       "Logistic Regression   0.500000  0.294872   0.125000  0.523810   0.523810   \n",
       "Decision Tree         0.547619  0.732510   0.500000  0.638889   0.500000   \n",
       "\n",
       "Feature                                        ZCR                       \\\n",
       "Metric               F1 Score             Accuracy             F1 Score   \n",
       "Set                  Training Validation  Training Validation  Training   \n",
       "SVM (Linear Kernel)  0.000000   0.000000  0.519841   0.535714  0.047244   \n",
       "SVM (RBF Kernel)     0.161765   0.046512  0.575397   0.619048  0.408840   \n",
       "Logistic Regression  0.016393   0.000000  0.547619   0.535714  0.366667   \n",
       "Decision Tree        0.616034   0.487805  0.626984   0.607143  0.591304   \n",
       "\n",
       "Feature                         \n",
       "Metric                          \n",
       "Set                 Validation  \n",
       "SVM (Linear Kernel)   0.048780  \n",
       "SVM (RBF Kernel)      0.428571  \n",
       "Logistic Regression   0.264151  \n",
       "Decision Tree         0.507463  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect results for all features\n",
    "all_results = {\n",
    "    'MFCC': mfcc_results,\n",
    "    'Pitch': pitch_results,\n",
    "    'Energy': energy_results,\n",
    "    'ZCR': zcr_results\n",
    "}\n",
    "\n",
    "# Prepare data for a multi-level column structure\n",
    "data = []\n",
    "models = list(all_results['MFCC'].keys())\n",
    "\n",
    "# Prepare data for combined table\n",
    "combined_data = []\n",
    "\n",
    "for model in models:\n",
    "    row = []\n",
    "    for feature, results in all_results.items():\n",
    "        metrics = results[model]\n",
    "        # Append Accuracy and F1 Score metrics (Training and Validation)\n",
    "        row.extend([\n",
    "            metrics['Training Accuracy'], metrics['Validation Accuracy'],  # Accuracy\n",
    "            metrics['Training F1 Score'], metrics['Validation F1 Score']   # F1 Score\n",
    "        ])\n",
    "    combined_data.append(row)\n",
    "\n",
    "# Define multi-level column names\n",
    "combined_columns = pd.MultiIndex.from_product(\n",
    "    [all_results.keys(), ['Accuracy', 'F1 Score'], ['Training', 'Validation']],\n",
    "    names=['Feature', 'Metric', 'Set']\n",
    ")\n",
    "\n",
    "# Create the DataFrame\n",
    "combined_summary_df = pd.DataFrame(combined_data, index=models, columns=combined_columns)\n",
    "\n",
    "# Display the combined summary table\n",
    "print(\"\\nCombined Training and Validation Metrics (Accuracy and F1 Scores) Across Features:\")\n",
    "display(combined_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Models for Each Feature (Considering Accuracy and F1 Score):\n",
      "MFCC: Best Model = SVM (RBF Kernel), Validation Accuracy = 0.73, Validation F1 Score = 0.69, Combined Score = 0.71\n",
      "Pitch: Best Model = Decision Tree, Validation Accuracy = 0.55, Validation F1 Score = 0.50, Combined Score = 0.52\n",
      "Energy: Best Model = Decision Tree, Validation Accuracy = 0.50, Validation F1 Score = 0.49, Combined Score = 0.49\n",
      "ZCR: Best Model = Decision Tree, Validation Accuracy = 0.61, Validation F1 Score = 0.51, Combined Score = 0.56\n"
     ]
    }
   ],
   "source": [
    "# Find the best model for each feature based on a combination of Accuracy and F1 Score\n",
    "best_models = {}\n",
    "for feature in all_results.keys():\n",
    "    # Extract validation Accuracy and F1 Scores for all models for the current feature\n",
    "    validation_accuracy = combined_summary_df[feature, 'Accuracy', 'Validation']\n",
    "    validation_f1_scores = combined_summary_df[feature, 'F1 Score', 'Validation']\n",
    "\n",
    "    # Calculate a combined score (e.g., average of Accuracy and F1 Score)\n",
    "    combined_scores = (validation_accuracy + validation_f1_scores) / 2\n",
    "\n",
    "    # Find the model with the highest combined score\n",
    "    best_model = combined_scores.idxmax()\n",
    "    best_combined_score = combined_scores.max()\n",
    "    best_accuracy = validation_accuracy[best_model]\n",
    "    best_f1_score = validation_f1_scores[best_model]\n",
    "\n",
    "    # Store results\n",
    "    best_models[feature] = {\n",
    "        'Model': best_model,\n",
    "        'Validation Accuracy': best_accuracy,\n",
    "        'Validation F1 Score': best_f1_score,\n",
    "        'Combined Score': best_combined_score\n",
    "    }\n",
    "\n",
    "# Display the best models for each feature\n",
    "print(\"\\nBest Models for Each Feature (Considering Accuracy and F1 Score):\")\n",
    "for feature, info in best_models.items():\n",
    "    print(f\"{feature}: Best Model = {info['Model']}, \"\n",
    "          f\"Validation Accuracy = {info['Validation Accuracy']:.2f}, \"\n",
    "          f\"Validation F1 Score = {info['Validation F1 Score']:.2f}, \"\n",
    "          f\"Combined Score = {info['Combined Score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and store models for each feature\n",
    "models_mfcc = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000).fit(X_mfcc_train, y_train),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42).fit(X_mfcc_train, y_train),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42).fit(X_mfcc_train, y_train),\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', C=1, gamma='scale', random_state=42).fit(X_mfcc_train, y_train)\n",
    "}\n",
    "\n",
    "models_pitch = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000).fit(X_pitch_train, y_train),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42).fit(X_pitch_train, y_train),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42).fit(X_pitch_train, y_train),\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', C=1, gamma='scale', random_state=42).fit(X_pitch_train, y_train)\n",
    "}\n",
    "\n",
    "models_energy = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000).fit(X_energy_train, y_train),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42).fit(X_energy_train, y_train),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42).fit(X_energy_train, y_train),\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', C=1, gamma='scale', random_state=42).fit(X_energy_train, y_train)\n",
    "}\n",
    "\n",
    "models_zcr = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000).fit(X_zcr_train, y_train),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42).fit(X_zcr_train, y_train),\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42).fit(X_zcr_train, y_train),\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', C=1, gamma='scale', random_state=42).fit(X_zcr_train, y_train)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Models for MFCC Feature:\n",
      "  Logistic Regression: Accuracy = 0.52, F1 Score = 0.46\n",
      "  Decision Tree: Accuracy = 0.70, F1 Score = 0.68\n",
      "  SVM (Linear Kernel): Accuracy = 0.61, F1 Score = 0.57\n",
      "  SVM (RBF Kernel): Accuracy = 0.73, F1 Score = 0.69\n",
      "\n",
      "Evaluating Models for Pitch Feature:\n",
      "  Logistic Regression: Accuracy = 0.50, F1 Score = 0.12\n",
      "  Decision Tree: Accuracy = 0.56, F1 Score = 0.55\n",
      "  SVM (Linear Kernel): Accuracy = 0.55, F1 Score = 0.14\n",
      "  SVM (RBF Kernel): Accuracy = 0.40, F1 Score = 0.17\n",
      "\n",
      "Evaluating Models for Energy Feature:\n",
      "  Logistic Regression: Accuracy = 0.52, F1 Score = 0.00\n",
      "  Decision Tree: Accuracy = 0.46, F1 Score = 0.47\n",
      "  SVM (Linear Kernel): Accuracy = 0.52, F1 Score = 0.00\n",
      "  SVM (RBF Kernel): Accuracy = 0.51, F1 Score = 0.05\n",
      "\n",
      "Evaluating Models for ZCR Feature:\n",
      "  Logistic Regression: Accuracy = 0.54, F1 Score = 0.26\n",
      "  Decision Tree: Accuracy = 0.58, F1 Score = 0.53\n",
      "  SVM (Linear Kernel): Accuracy = 0.54, F1 Score = 0.05\n",
      "  SVM (RBF Kernel): Accuracy = 0.62, F1 Score = 0.43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_models(models, X_val, y_val, feature_name):\n",
    "    \"\"\"\n",
    "    Evaluate all models for a specific feature on the validation set.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary of trained models for the feature.\n",
    "        X_val (np.ndarray): Validation features for the feature.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "        feature_name (str): Name of the feature being evaluated.\n",
    "\n",
    "    Returns:\n",
    "        dict: Performance metrics (Accuracy and F1 Score) for all models.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    print(f\"\\nEvaluating Models for {feature_name} Feature:\")\n",
    "    for model_name, model in models.items():\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        f1 = f1_score(y_val, y_val_pred)\n",
    "        results[model_name] = {\n",
    "            'Validation Accuracy': acc,\n",
    "            'Validation F1 Score': f1\n",
    "        }\n",
    "        print(f\"  {model_name}: Accuracy = {acc:.2f}, F1 Score = {f1:.2f}\")\n",
    "    return results\n",
    "\n",
    "# Evaluate all models for each feature\n",
    "mfcc_results = evaluate_models(models_mfcc, X_mfcc_val, y_val, 'MFCC')\n",
    "pitch_results = evaluate_models(models_pitch, X_pitch_val, y_val, 'Pitch')\n",
    "energy_results = evaluate_models(models_energy, X_energy_val, y_val, 'Energy')\n",
    "zcr_results = evaluate_models(models_zcr, X_zcr_val, y_val, 'ZCR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model for MFCC Feature: SVM (RBF Kernel) (Accuracy = 0.73, F1 Score = 0.69)\n",
      "\n",
      "Best Model for Pitch Feature: Decision Tree (Accuracy = 0.56, F1 Score = 0.55)\n",
      "\n",
      "Best Model for Energy Feature: Decision Tree (Accuracy = 0.46, F1 Score = 0.47)\n",
      "\n",
      "Best Model for ZCR Feature: Decision Tree (Accuracy = 0.58, F1 Score = 0.53)\n"
     ]
    }
   ],
   "source": [
    "# Function to find the best model based on Validation F1 Score\n",
    "def find_best_model(results, feature_name):\n",
    "    \"\"\"\n",
    "    Find the best model for a specific feature based on Validation F1 Score.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Evaluation results for all models of a feature.\n",
    "        feature_name (str): Name of the feature.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best model details (name, metrics).\n",
    "    \"\"\"\n",
    "    best_model_name = None\n",
    "    best_metrics = None\n",
    "    best_f1 = 0\n",
    "\n",
    "    for model_name, metrics in results.items():\n",
    "        if metrics['Validation F1 Score'] > best_f1:\n",
    "            best_model_name = model_name\n",
    "            best_metrics = metrics\n",
    "            best_f1 = metrics['Validation F1 Score']\n",
    "\n",
    "    print(f\"\\nBest Model for {feature_name} Feature: {best_model_name} \"\n",
    "          f\"(Accuracy = {best_metrics['Validation Accuracy']:.2f}, \"\n",
    "          f\"F1 Score = {best_metrics['Validation F1 Score']:.2f})\")\n",
    "    return {\n",
    "        'Model Name': best_model_name,\n",
    "        'Metrics': best_metrics\n",
    "    }\n",
    "\n",
    "# Identify the best model for each feature\n",
    "best_models = {\n",
    "    'MFCC': find_best_model(mfcc_results, 'MFCC'),\n",
    "    'Pitch': find_best_model(pitch_results, 'Pitch'),\n",
    "    'Energy': find_best_model(energy_results, 'Energy'),\n",
    "    'ZCR': find_best_model(zcr_results, 'ZCR')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Performance (Majority Voting):\n",
      "  Accuracy: 0.58\n",
      "  F1 Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect predictions from the best models for validation set\n",
    "mfcc_preds = models_mfcc[best_models['MFCC']['Model Name']].predict(X_mfcc_val)\n",
    "pitch_preds = models_pitch[best_models['Pitch']['Model Name']].predict(X_pitch_val)\n",
    "energy_preds = models_energy[best_models['Energy']['Model Name']].predict(X_energy_val)\n",
    "zcr_preds = models_zcr[best_models['ZCR']['Model Name']].predict(X_zcr_val)\n",
    "\n",
    "# Combine predictions (Majority Voting)\n",
    "all_preds = np.array([mfcc_preds, pitch_preds, energy_preds, zcr_preds])\n",
    "ensemble_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_preds)\n",
    "\n",
    "# Evaluate Ensemble\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble_preds)\n",
    "ensemble_f1 = f1_score(y_val, ensemble_preds)\n",
    "\n",
    "print(\"\\nEnsemble Performance (Majority Voting):\")\n",
    "print(f\"  Accuracy: {ensemble_accuracy:.2f}\")\n",
    "print(f\"  F1 Score: {ensemble_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Models for Each Feature (Considering Accuracy and F1 Score):\n",
      "MFCC: Best Model = SVM (RBF Kernel), Validation Accuracy = 0.73, Validation F1 Score = 0.69, Combined Score = 0.71\n",
      "Pitch: Best Model = Decision Tree, Validation Accuracy = 0.55, Validation F1 Score = 0.50, Combined Score = 0.52\n",
      "Energy: Best Model = Decision Tree, Validation Accuracy = 0.50, Validation F1 Score = 0.49, Combined Score = 0.49\n",
      "ZCR: Best Model = Decision Tree, Validation Accuracy = 0.61, Validation F1 Score = 0.51, Combined Score = 0.56\n"
     ]
    }
   ],
   "source": [
    "# Define models dictionary with actual trained models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000,random_state=42),  # Replace with your trained Logistic Regression model\n",
    "    'Decision Tree':DecisionTreeClassifier(random_state=42),  # Replace with your trained Decision Tree model\n",
    "    'SVM (Linear Kernel)': SVC(kernel='linear', random_state=42),  # Replace with your trained SVM (Linear Kernel) model\n",
    "    'SVM (RBF Kernel)': SVC(kernel='rbf', C=1, gamma='scale', random_state=42) # Replace with your trained SVM (RBF Kernel) model\n",
    "}\n",
    "\n",
    "# Find the best model for each feature based on combined metrics\n",
    "best_models = {}\n",
    "for feature, results in all_results.items():\n",
    "    best_model_name = None\n",
    "    best_model_object = None\n",
    "    best_combined_score = 0\n",
    "\n",
    "    for model_name, metrics in results.items():\n",
    "        # Calculate combined score (average of Accuracy and F1 Score)\n",
    "        combined_score = (metrics['Validation Accuracy'] + metrics['Validation F1 Score']) / 2\n",
    "\n",
    "        if combined_score > best_combined_score:\n",
    "            best_combined_score = combined_score\n",
    "            best_model_name = model_name\n",
    "            best_model_object = models[model_name]  # Retrieve the trained model from the dictionary\n",
    "\n",
    "    # Save the best model details\n",
    "    best_models[feature] = {\n",
    "        'Model Name': best_model_name,\n",
    "        'Model': best_model_object,  # Store the actual trained model object\n",
    "        'Validation Accuracy': results[best_model_name]['Validation Accuracy'],\n",
    "        'Validation F1 Score': results[best_model_name]['Validation F1 Score'],\n",
    "        'Combined Score': best_combined_score\n",
    "    }\n",
    "\n",
    "# Display the best models for each feature\n",
    "print(\"\\nBest Models for Each Feature (Considering Accuracy and F1 Score):\")\n",
    "for feature, info in best_models.items():\n",
    "    print(f\"{feature}: Best Model = {info['Model Name']}, \"\n",
    "          f\"Validation Accuracy = {info['Validation Accuracy']:.2f}, \"\n",
    "          f\"Validation F1 Score = {info['Validation F1 Score']:.2f}, \"\n",
    "          f\"Combined Score = {info['Combined Score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "def majority_voting_ensemble(X_train, X_val, y_train, y_val, models):\n",
    "    \"\"\"\n",
    "    Perform majority voting ensemble using VotingClassifier.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training feature set.\n",
    "        X_val (np.ndarray): Validation feature set.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        y_val (np.ndarray): Validation labels.\n",
    "        models (list of tuples): List of (name, model) pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Accuracy and F1 Score of the ensemble on validation data.\n",
    "    \"\"\"\n",
    "    # Create a VotingClassifier\n",
    "    ensemble_model = VotingClassifier(estimators=models, voting='hard')\n",
    "    ensemble_model.fit(X_train, y_train)  # Train the ensemble model\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = ensemble_model.predict(X_val)\n",
    "\n",
    "    # Compute metrics\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    print(\"\\nMajority Voting Ensemble Performance:\")\n",
    "    print(f\"  Accuracy: {val_acc:.2f}\")\n",
    "    print(f\"  F1 Score: {val_f1:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'Validation Accuracy': val_acc,\n",
    "        'Validation F1 Score': val_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Majority Voting Ensemble Performance:\n",
      "  Accuracy: 0.49\n",
      "  F1 Score: 0.43\n"
     ]
    }
   ],
   "source": [
    "# Prepare individual models for the ensemble\n",
    "models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000,random_state=42)),  # Best logistic regression model\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),  # Best decision tree model\n",
    "    ('svm_linear', SVC(kernel='linear', random_state=42)),  # Best SVM (linear kernel)\n",
    "    ('svm_rbf', SVC(kernel='rbf', C=1, gamma='scale', random_state=42))  # Best SVM (RBF kernel)\n",
    "]\n",
    "\n",
    "# Perform the ensemble\n",
    "ensemble_results = majority_voting_ensemble(X_train, X_val, y_train, y_val, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def feature_level_ensemble(predictions, y_val):\n",
    "    \"\"\"\n",
    "    Perform feature-level ensemble using majority voting.\n",
    "\n",
    "    Args:\n",
    "        predictions (list of np.ndarray): List of predictions from models trained on different features.\n",
    "        y_val (np.ndarray): Ground truth labels for the validation set.\n",
    "\n",
    "    Returns:\n",
    "        dict: Accuracy and F1 Score of the ensemble on validation data.\n",
    "    \"\"\"\n",
    "    # Stack predictions and compute majority vote for each sample\n",
    "    predictions_array = np.array(predictions)\n",
    "    ensemble_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions_array)\n",
    "\n",
    "    # Compute metrics\n",
    "    val_acc = accuracy_score(y_val, ensemble_preds)\n",
    "    val_f1 = f1_score(y_val, ensemble_preds)\n",
    "\n",
    "    print(\"\\nFeature-Level Ensemble Performance:\")\n",
    "    print(f\"  Accuracy: {val_acc:.2f}\")\n",
    "    print(f\"  F1 Score: {val_f1:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'Validation Accuracy': val_acc,\n",
    "        'Validation F1 Score': val_f1\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
