{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91MsGMTna_P9"
   },
   "source": [
    "# ECS7020P mini-project submission\n",
    "\n",
    "\n",
    "## What is the problem?\n",
    "\n",
    "This year's mini-project considers the problem of predicting whether a narrated story is true or not. Specifically, you will build a machine learning model that takes as an input an audio recording of **30 seconds** of duration and predicts whether the story being narrated is **true or not**. \n",
    "\n",
    "\n",
    "## Which dataset will I use?\n",
    "\n",
    "A total of 100 samples consisting of a complete audio recording, a *Language* attribute and a *Story Type* attribute have been made available for you to build your machine learning model. The audio recordings can be downloaded from:\n",
    "\n",
    "https://github.com/MLEndDatasets/Deception/tree/main/MLEndDD_stories_small\n",
    "\n",
    "A CSV file recording the *Language* attribute and *Story Type* of each audio file can be downloaded from:\n",
    "\n",
    "https://github.com/MLEndDatasets/Deception/blob/main/MLEndDD_story_attributes_small.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What will I submit?\n",
    "\n",
    "Your submission will consist of **one single Jupyter notebook** that should include:\n",
    "\n",
    "*   **Text cells**, describing in your own words, rigorously and concisely your approach, each implemented step and the results that you obtain,\n",
    "*   **Code cells**, implementing each step,\n",
    "*   **Output cells**, i.e. the output from each code cell,\n",
    "\n",
    "Your notebook **should have the structure** outlined below. Please make sure that you **run all the cells** and that the **output cells are saved** before submission. \n",
    "\n",
    "Please save your notebook as:\n",
    "\n",
    "* ECS7020P_miniproject_2425.ipynb\n",
    "\n",
    "\n",
    "## How will my submission be evaluated?\n",
    "\n",
    "This submission is worth 16 marks. We will value:\n",
    "\n",
    "*   Conciseness in your writing.\n",
    "*   Correctness in your methodology.\n",
    "*   Correctness in your analysis and conclusions.\n",
    "*   Completeness.\n",
    "*   Originality and efforts to try something new.\n",
    "\n",
    "**The final performance of your solutions will not influence your grade**. We will grade your understanding. If you have an good understanding, you will be using the right methodology, selecting the right approaches, assessing correctly the quality of your solutions, sometimes acknowledging that despite your attempts your solutions are not good enough, and critically reflecting on your work to suggest what you could have done differently. \n",
    "\n",
    "Note that **the problem that we are intending to solve is very difficult**. Do not despair if you do not get good results, **difficulty is precisely what makes it interesting** and **worth trying**. \n",
    "\n",
    "## Show the world what you can do \n",
    "\n",
    "Why don't you use **GitHub** to manage your project? GitHub can be used as a presentation card that showcases what you have done and gives evidence of your data science skills, knowledge and experience. **Potential employers are always looking for this kind of evidence**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------- PLEASE USE THE STRUCTURE BELOW THIS LINE --------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Audio Features for Deceptive Storytelling Detection Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**:  Erica Low Ee Zhin\n",
    "\n",
    "**Student ID**: 231126519\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Problem Formulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the machine learning problem that you want to solve and explain what's interesting about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting deception is a vital yet complex challenge, with applications across various fields, ranging from law enforcement, police investigation, private consulting to psychological research.[1] Traditional lie detection methods, such as polygraphs tests, have significant limitations as they rely on physiological measurements, like heartbeat, blood pressure, respiration and skin temperature. [1][2] This approach often fail to capture the nuanced nature of human deception, as one can supress physical signs of discomfort, leading to false conclusions. While previous research has extensively explored visual and behavioral cues, such as eye blink analysis, pupil dilation and facial expression analysis [2], there is limited work on predicting deception purely based on speech data. \n",
    "\n",
    "With this, this project aims to fill this gap by developing a machine learning model to determine whether a narrated story is truthful or deceptive based soley on audio features. Raw audio signals, being high-dimensional data, pose challenges for direct analysis, as we will be operating in a predictor space consisting of hundreds of thousands of dimensions. To address this, we extract a set of four key audio features - Power, Pitch Mean, Pitch Standard Deviation, and Voice Fraction, which are extracted from 30-second segments of audio recordings. These features provide a manageable and informative predictor space for the model. Unlike polygraph tests, this non-invasive approach only requires only a short audio sample and can be performed without the need for specialized expertise such as polygraphists or psychologists. In addition to that, this approach also eliminates the reliance on physiological measures, providing a portable, more easily accessible tool for deception detection that individuals cannot easily manipulate. \n",
    "\n",
    "The problem of deceptive storytelling detection is framed as a binary classification task, with the labels of `true_story` and `deceptive_story`. However, the inherent variability in audio data, including differences in languages, accents, and speaking styles, makes this task particularly complex. To overcome these challenges, this project utilises ensemble classification techniques, by combining multiple classifiers models, aiming to improve the final predictive performance and create a reliable machine learning pipleline for detecting audio-based deception. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "Describe your methodology. Specifically, describe your training task and validation task, and how model performance is defined (i.e. accuracy, confusion matrix, etc). Any other tasks that might help you build your model should also be described here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Data Preprocessing**\n",
    "- All 100 audio recordings were divided into several 30-second chunks based on the original audio file length to standardize input lengths for the machine learning models. Any chunks shorter than 30 seconds were considered invalid and discarded.\n",
    "- Each chunk was assigned a unique identifier (e.g., 00001.wav_chunk1), and chunks from the same recording were given the same File ID.\n",
    "- The sampling rate of each chunk was identified and recorded to ensure compatibility across the dataset.\n",
    "\n",
    "\n",
    "#### **3.2 Feature Extraction and Labels** \n",
    "- From each 30-second chunk, the following four audio features were extracted: \n",
    "  - **Power** : Represents the loudness of the audio.\n",
    "  - **Pitch Mean** : Meaures the average highness or lowness of the sound.\n",
    "  - **Pitch Standard Deviation** : Measures the pitch variability, distinguishing steady tones and shaky tones \n",
    "  - **Fraction of voiced region.** : Represents the proportion of time the audio contains voiced sounds. \n",
    "- The extracted features were combined into a single predictor matrix, where each row corresponds to a chunk and each column represents a feature.\n",
    "- The lables for each 30 second chunks were binary, where 0 represents `true_story` and `1` represents `deception_story`. Class distribution was carefully monitored to maintain balance during preprocessing and splitting.\n",
    "\n",
    "\n",
    "#### **3.3 Training Task**\n",
    "- Before fitting into classifiers models, all of the features were standardized using `StandardScaler` to normalize data for consistency across models.\n",
    "- Using `StratifiedGroupKFold`, the whole dataset was then split into training set (70%) and validation set (30%). This is to ensure both balanced class distribution in both sets and prevention of data leakage by ensuring chunks from the same audio file were not present in both training and validation sets.\n",
    "- The training set consisted of 276 chunks, and the validation set contained 144 chunks. Both sets had a similar distribution of labels: approximately 48% true_story and 52% deceptive_story.\n",
    "\n",
    "#### **3.4 Validation Task**\n",
    "- The validation task aimed to assess model performance on unseen data while maintaining class balance. The validation set was evaluated after training on the following metrics:\n",
    "  - **Accuracy**: The proportion of correct predictions.\n",
    "  - **F1 Score**: A balanced measure of precision and recall to account for class imbalance.\n",
    "\n",
    "  Accuracy: This metric measures the overall correctness of predictions and provides a general sense of model performance.\n",
    "F1 Score: This metric was prioritized, as it balances precision and recall, making it particularly suitable for this task where both false positives and false negatives carry significant implications.\n",
    "  \n",
    "#### **3.5 Model Training**\n",
    "- A total of four classifiers models were trained and evaluated, Logistic Regression, Random Forest, Gradient Boosting and Support Vector Machine (SVM).\n",
    "\n",
    "- Each model's performance was assessed using training and validation of accuracy and F1 scores.\n",
    "\n",
    "- A soft-voting ensemble classifier was created by combining the two best-performing models (based on validation F1 scores). The ensemble model was evaluated using validation accuracy and validation F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BwrtEdLDit"
   },
   "source": [
    "# 4 Implemented ML prediction pipelines\n",
    "\n",
    "Describe the ML prediction pipelines that you will explore. Clearly identify their input and output, stages and format of the intermediate data structures moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. After providing an overview, describe in more detail each one of the stages that you have included in their corresponding subsections (i.e. 4.1 Transformation stage, 4.2 Model stage, 4.3 Ensemble stage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the overview of the machine learning (ML) prediction pipelines implemented for the audio-based deception detection. These pipelines consist of multiple stages, including: \n",
    "- 4.1 Feature Transformation\n",
    "- 4.2 Model Training\n",
    "- 4.3 Ensemble Learning\n",
    "\n",
    "#### **Pipeline Overview**\n",
    "**1. Input:** \n",
    "\n",
    "100 of raw audio recordings, with each individual recording is splitted into 30-second chunks. Each chunk is labeled as either true_story (0) or deceptive_story (1). These audio chunks serve as the primary input, providing the basis for subsequent feature extraction and model training. The data format at this stage includes:\n",
    "- Unique identifiers for each chunk (e.g., FileID_ChunkID).\n",
    "- Labels (true_story or deceptive_story) to guide supervised learning.\n",
    "\n",
    "**2. Transformation Stage:** \n",
    "\n",
    "In this stage, the raw audio data is transformed into a structured feature matrix that can be used for machine learning. Key steps include:\n",
    "- Feature Extraction: A total of four audio features were extracted, including power, pitch mean, pitch standard deviation, and voiced fraction—from each chunk. These features are selected for their relevance to detecting deceptive behavior.\n",
    "\n",
    "- Feature Standardization: Using a StandardScaler to normalize feature values, ensuring consistent scales across the dataset.\n",
    "\n",
    "The output of this stage is a feature matrix, where rows represent individual audio chunks and columns correspond to the extracted features, file ID and labels.\n",
    "\n",
    "**3. Model Stage:**\n",
    "\n",
    "Next, the feature matrix were splitted into 70% of training datasets and 30% of validation datasets. Then, the standardized features were trained with various ML classifiers models to learn the patterns associated with truthful and deceptive stories. The classifiers are Logistic Regression, Random Forest, Gradient Boosting, and SVM. Then, each model was evaluated using accuracy and F1 scores on both training and validation datasets to look at their predictive performance and generalizability.\n",
    "\n",
    "The output of this stage is a dataframe which concludes each model evaluation metrisc of accuracy and F1 scores on both training and validation datasets.\n",
    "\n",
    "**4. Ensemble Stage:**\n",
    "\n",
    "In this stage, the top two best performing models (based on validation F1 scores) are combined into an ensemble using a soft-voting approach. Then this ensembled model is then trained and evaluated on the same set of training and validation sets as the individual models previously. \n",
    "\n",
    "The output of this stage is the validation accuracy and F1 score of the ensemble model and this results are compared with the individual model performace to check the capability of ensemble models in improving deceptive storytelling detection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1 Transformation Stage**\n",
    "\n",
    "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage.\n",
    "\n",
    "This section outlines the transformation stage, detailing the input and output, and explaining the rationale behind the chosen preprocessing and feature extraction methods.\n",
    "\n",
    "### **4.1.1 Input:**\n",
    "- 100 raw audio files labeled with their corresponding File ID, Language, and Story Type (true_story or deceptive_story), stored in the `MLEND_df` dataframe.\n",
    "\n",
    "### **4.1.2 Audio Splitting:**\n",
    "- Each raw audio file is divided into 30-second chunks to standardize input length for feature extraction and model training.\n",
    "- Chunks shorter than 30 seconds are discarded to ensure consistency in input duration and to prevent bias caused by shorter chunks having different statistical properties.\n",
    "- Metadata such as total duration, sample rate, total number of chunks, valid chunks are recorded.\n",
    "- Intermediate output: A `file_metadata` dataframe that summarizes the raw audio statistics for each file, including File ID, Duration(s), Sample Rate, Total Chunks, Valid Chunks (30s), and Story Type. Another dataframe, `chunk_df` containing only valid 30s chunks, with columns for File ID, Chunk ID, and Story Type.\n",
    "\n",
    "### **4.1.3 Features Extraction and Labelling:**\n",
    "- In this section, each valid 30s chunks in the `chunk_df` undergo feature extraction to derive four key audio features which capture the audio patterns relevant to deception storytelling detection, which are: \n",
    "  - **Power**: Measures the energy of the audio signal to capture intensity patterns.\n",
    "  - **Pitch Mean and Standard Deviation**: Statistical measures of pitch to capture frequency variations relevant to storytelling dynamics.\n",
    "  - **Voiced Fraction**: Proportion of time the audio contains voiced sounds, which is important for speech analysis.\n",
    "-The story types (true_story and deceptive_story) are encoded as label of 0 and 1, respectively in each valid chunks.\n",
    "\n",
    "### **4.1.4 Output:**\n",
    "- A feature matrix saved as `extracted_features.csv`, where each row represents a valid chunk, with columns for the four features, File ID, and the corresponding label.\n",
    "This CSV storage is to reduce processing time by avoiding redundant feature extraction during multiple runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "## **4.2 Model Stage**\n",
    "Describe the ML model(s) that you will build. Explain why you have chosen them.\n",
    "\n",
    "Four machine learning models were selected for training and evaluation, each offering unique strengths and capabilities to ensure a generalised and robust analysis:\n",
    "\n",
    "### **4.2.1 Machine Learning Models**:\n",
    "- **Logistic Regression**: \n",
    "    - A simple linear model, served as a baseline to assess whether the audio dataset is linearly seperable. \n",
    "    - It is easily interpretable and offers quick implementation, which facilitates direct comparison with more complex models.\n",
    "- **Random Forest**: \n",
    "    - A non-linear ensemble model based on decision trees, which able to detect non-linear relationships.\n",
    "    - The parameters were configured with `max_depth`=3, `n_estimators`=10, to prevent overfitting, ensuring that the model does not memorise training data while maintaining its predictive power. \n",
    "- **Gradient Boosting**: \n",
    "    - An iterative model that builds strong classifiers by combining weak learners to optimise the model performance over several iterations. This model is suitable for imbalanced datasets like our audio datasets which has 52% of true stories and 48% of deceptive stories in both training and validation dataset.\n",
    "    - The parameter was configured with `max_depth`=3, and `n_estimators`=30, to prevent overfitting. \n",
    "- **Support Vector Machine (SVM)**: \n",
    "    - A powerful classifier with good performance for smaller datasets and higher-dimensional feature spaces, which suitables our audio dataset (420 audio files and high dimensional audio signals).\n",
    "    - The model uses an \"rbf\" kernel, wuth C=3, and the gamma =\"scale\", to construct non-linear boundaries which improves its ability to seperate the two classes effectively.\n",
    "    - \"rbf\" kernel is used as the extracted features (power, pitch mean/std, voiced fraction) are likely to exhibit non-linear relationships that distinguish true and deceptive storytelling. As both truth and deceptive speech might involve complex combinations of pitch variation and energy that a linear decision boundary cannot effectively separate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Ensemble Stage**\n",
    "Describe any ensemble approach you might have included. Explain why you have chosen them.\n",
    "\n",
    "This section introduces the ensemble approach utilized in this project, explaining its methodology and the rationale for selecting it to enhance overall model predictive performance.\n",
    "\n",
    "### **4.3.1 Soft Voting Ensemble:**\n",
    "- The top two best model performance (based on Validation F1 score) was chosen and by utilising soft voting method, the class prediction probabilities from the selected models was averaged and hence predicting the final class probabilities. \n",
    "\n",
    "### **4.3.2 Reasons of using this approach:**\n",
    "- Unlike hard voting, soft voting ensemble could leverage the strengths of each models, balancing the strengths of each models, creating a more robust-decision making process which is useful in this deception storytelling detection, which involving complex relationships in the audio features.\n",
    "- By averaging the probabilities, the ensemble models are less sensitive to overconfident predictions from single model. \n",
    "- Soft voting ensembles combines different perspectives, leading to a more reliable final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n",
    "Describe the datasets that you will create to build and evaluate your models. Your datasets need to be based on our MLEnd Deception Dataset. After describing the datasets, build them here. You can explore and visualise the datasets here as well. \n",
    "\n",
    "If you are building separate training and validation datasets, do it here. Explain clearly how you are building such datasets, how you are ensuring that they serve their purpose (i.e. they are independent and consist of ID samples) and any limitations you might think of. It is always important to identify any limitations as early as possible. The scope and validity of your conclusions will depend on your ability to understand the limitations of your approach.\n",
    "\n",
    "If you are exploring different datasets, create different subsections for each dataset and give them a name (e.g. 5.1 Dataset A, 5.2 Dataset B, 5.3 Dataset 5.3) .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100 audio files, with varies length with each is at least 2 minutes and above \n",
    "- 50 truth and 50 deceptive story\n",
    "- have different languages - english (78 number), and other langusges( ? numbers) Hindi' 'Bengali' 'Kannada' 'French' 'Arabic' 'Russian'\n",
    " 'Chinese, Mandarin' 'Marathi' 'Portuguese' 'Spanish' 'Swahilli' 'Telugu'\n",
    " 'Korean' 'Cantonese' 'Italian'\n",
    "\n",
    "- normalised all recordings into 30 seconds only as per required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets for this project are based on the MLEnd Deception Dataset,which can be installed in the following four steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Step 1: Install the Required Library##\n",
    "# pip install mlend==1.0.0.4\n",
    "\n",
    "##Step 2: Import Library and Functions##\n",
    "# import mlend\n",
    "# from mlend import download_deception_small, deception_small_load\n",
    "\n",
    "##Step 3: Download small data##\n",
    "# datadir = download_deception_small(save_to='MLEnd', subset={}, verbose=1, overwrite=False)\n",
    "\n",
    "##Step 4: Read file paths##\n",
    "# base_path = './MLEnd/deception/MLEndDD_stories_small/'\n",
    "# MLEND_df = pd.read_csv('./MLEnd/deception/MLEndDD_story_attributes_small.csv').set_index('filename')\n",
    "# files = [base_path + file for file in MLEND_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLEnd Deception Dataset comprises 100 raw audio recordings of narrated stories, varying in language and duration, with each recording labeled as either a `true_story` or `deceptive_story`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Story_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001.wav</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00002.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00003.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004.wav</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00005.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Language       Story_type\n",
       "filename                           \n",
       "00001.wav    Hindi  deceptive_story\n",
       "00002.wav  English       true_story\n",
       "00003.wav  English  deceptive_story\n",
       "00004.wav  Bengali  deceptive_story\n",
       "00005.wav  English  deceptive_story"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 100 audio files in the dataset.\n",
      "\n",
      "Languages narrated in the dataset are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Language</th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Chinese, Mandarin</th>\n",
       "      <th>Marathi</th>\n",
       "      <th>Bengali</th>\n",
       "      <th>Kannada</th>\n",
       "      <th>French</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Swahilli</th>\n",
       "      <th>Telugu</th>\n",
       "      <th>Korean</th>\n",
       "      <th>Cantonese</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Language  English  Hindi  Arabic  Chinese, Mandarin  Marathi  Bengali  \\\n",
       "count          78      4       3                  2        2        1   \n",
       "\n",
       "Language  Kannada  French  Russian  Portuguese  Spanish  Swahilli  Telugu  \\\n",
       "count           1       1        1           1        1         1       1   \n",
       "\n",
       "Language  Korean  Cantonese  Italian  Sum  \n",
       "count          1          1        1  100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story_type\n",
      "deceptive_story    50\n",
      "true_story         50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Data Loading \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = './MLEnd/deception/MLEndDD_stories_small/'\n",
    "MLEND_df = pd.read_csv('./MLEnd/deception/MLEndDD_story_attributes_small.csv').set_index('filename')\n",
    "files = [base_path + file for file in MLEND_df.index]\n",
    "\n",
    "display(MLEND_df.head())\n",
    "print(f\"We have a total of {len(files)} audio files in the dataset.\")\n",
    "\n",
    "\n",
    "#Langauge Distribution\n",
    "language_counts = MLEND_df['Language'].value_counts()\n",
    "language_df = pd.DataFrame(language_counts).transpose()\n",
    "language_df['Sum'] = language_counts.sum()\n",
    "print(\"\\nLanguages narrated in the dataset are:\")\n",
    "display(language_df)\n",
    "\n",
    "#Data Distribution\n",
    "story_type_counts = MLEND_df['Story_type'].value_counts()\n",
    "print(story_type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results above, we have a balanced dataset of 50 true stores and the remainining 50 narrated stories are deceptive. The 100 recordings were narrated in a total of 16 different languages as shown in `langugaes_df` above, where English is the dominant language of the narrated stories of a total 78 recordings, the followed by other languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create two dataframes of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Audio Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Total Duration (s)</th>\n",
       "      <th>Total Chunks</th>\n",
       "      <th>Valid Chunks</th>\n",
       "      <th>Story Type</th>\n",
       "      <th>Original Sample Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>122.167256</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>125.192018</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>true_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.wav</td>\n",
       "      <td>162.984127</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.wav</td>\n",
       "      <td>121.681270</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.wav</td>\n",
       "      <td>134.189751</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>00096.wav</td>\n",
       "      <td>111.512063</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>00097.wav</td>\n",
       "      <td>185.731224</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>true_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>00098.wav</td>\n",
       "      <td>128.252766</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>00099.wav</td>\n",
       "      <td>132.412562</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>true_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>00100.wav</td>\n",
       "      <td>123.273560</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      File ID  Total Duration (s)  Total Chunks  Valid Chunks  \\\n",
       "0   00001.wav          122.167256             5             4   \n",
       "1   00002.wav          125.192018             5             4   \n",
       "2   00003.wav          162.984127             6             5   \n",
       "3   00004.wav          121.681270             5             4   \n",
       "4   00005.wav          134.189751             5             4   \n",
       "..        ...                 ...           ...           ...   \n",
       "95  00096.wav          111.512063             4             3   \n",
       "96  00097.wav          185.731224             7             6   \n",
       "97  00098.wav          128.252766             5             4   \n",
       "98  00099.wav          132.412562             5             4   \n",
       "99  00100.wav          123.273560             5             4   \n",
       "\n",
       "         Story Type  Original Sample Rate  \n",
       "0   deceptive_story                 44100  \n",
       "1        true_story                 44100  \n",
       "2   deceptive_story                 44100  \n",
       "3   deceptive_story                 44100  \n",
       "4   deceptive_story                 44100  \n",
       "..              ...                   ...  \n",
       "95  deceptive_story                 44100  \n",
       "96       true_story                 44100  \n",
       "97  deceptive_story                 44100  \n",
       "98       true_story                 44100  \n",
       "99  deceptive_story                 44100  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Valid Audio Chunks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Chunk Data</th>\n",
       "      <th>Story Type</th>\n",
       "      <th>Sample Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk1</td>\n",
       "      <td>[1.5258789e-05, 1.5258789e-05, 3.0517578e-05, ...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk2</td>\n",
       "      <td>[0.027450562, 0.026519775, 0.025390625, 0.0242...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk3</td>\n",
       "      <td>[-0.00091552734, -0.0011138916, -0.0013122559,...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk4</td>\n",
       "      <td>[6.1035156e-05, 9.1552734e-05, 7.6293945e-05, ...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>00002.wav_chunk1</td>\n",
       "      <td>[0.0008239746, 0.0008239746, 0.00088500977, 0....</td>\n",
       "      <td>true_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>00099.wav</td>\n",
       "      <td>00099.wav_chunk4</td>\n",
       "      <td>[-3.0517578e-05, -3.0517578e-05, -3.0517578e-0...</td>\n",
       "      <td>true_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>00100.wav</td>\n",
       "      <td>00100.wav_chunk1</td>\n",
       "      <td>[-0.00018310547, -0.00015258789, -6.1035156e-0...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>00100.wav</td>\n",
       "      <td>00100.wav_chunk2</td>\n",
       "      <td>[0.0004272461, 0.00048828125, 0.0005187988, 0....</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>00100.wav</td>\n",
       "      <td>00100.wav_chunk3</td>\n",
       "      <td>[6.1035156e-05, 0.0, -6.1035156e-05, -6.103515...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>00100.wav</td>\n",
       "      <td>00100.wav_chunk4</td>\n",
       "      <td>[3.0517578e-05, 0.0, 3.0517578e-05, 0.00012207...</td>\n",
       "      <td>deceptive_story</td>\n",
       "      <td>44100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       File ID          Chunk ID  \\\n",
       "0    00001.wav  00001.wav_chunk1   \n",
       "1    00001.wav  00001.wav_chunk2   \n",
       "2    00001.wav  00001.wav_chunk3   \n",
       "3    00001.wav  00001.wav_chunk4   \n",
       "4    00002.wav  00002.wav_chunk1   \n",
       "..         ...               ...   \n",
       "415  00099.wav  00099.wav_chunk4   \n",
       "416  00100.wav  00100.wav_chunk1   \n",
       "417  00100.wav  00100.wav_chunk2   \n",
       "418  00100.wav  00100.wav_chunk3   \n",
       "419  00100.wav  00100.wav_chunk4   \n",
       "\n",
       "                                            Chunk Data       Story Type  \\\n",
       "0    [1.5258789e-05, 1.5258789e-05, 3.0517578e-05, ...  deceptive_story   \n",
       "1    [0.027450562, 0.026519775, 0.025390625, 0.0242...  deceptive_story   \n",
       "2    [-0.00091552734, -0.0011138916, -0.0013122559,...  deceptive_story   \n",
       "3    [6.1035156e-05, 9.1552734e-05, 7.6293945e-05, ...  deceptive_story   \n",
       "4    [0.0008239746, 0.0008239746, 0.00088500977, 0....       true_story   \n",
       "..                                                 ...              ...   \n",
       "415  [-3.0517578e-05, -3.0517578e-05, -3.0517578e-0...       true_story   \n",
       "416  [-0.00018310547, -0.00015258789, -6.1035156e-0...  deceptive_story   \n",
       "417  [0.0004272461, 0.00048828125, 0.0005187988, 0....  deceptive_story   \n",
       "418  [6.1035156e-05, 0.0, -6.1035156e-05, -6.103515...  deceptive_story   \n",
       "419  [3.0517578e-05, 0.0, 3.0517578e-05, 0.00012207...  deceptive_story   \n",
       "\n",
       "     Sample Rate  \n",
       "0          44100  \n",
       "1          44100  \n",
       "2          44100  \n",
       "3          44100  \n",
       "4          44100  \n",
       "..           ...  \n",
       "415        44100  \n",
       "416        44100  \n",
       "417        44100  \n",
       "418        44100  \n",
       "419        44100  \n",
       "\n",
       "[420 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "Total Files Processed: 100\n",
      "Total Chunks Created: 520\n",
      "Total Valid Chunks (30s): 420\n",
      "Unique Sample Rates: [44100 48000]\n",
      "\n",
      "Count of True and Deceptive Stories from Valid Chunks (30s):\n",
      "true_story: 219 chunks\n",
      "deceptive_story: 201 chunks\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "metadata_rows = []\n",
    "chunk_rows = []\n",
    "\n",
    "for file_id in tqdm(MLEND_df.index):\n",
    "    file_path = base_path + file_id\n",
    "    story_type = MLEND_df.loc[file_id, 'Story_type']\n",
    "\n",
    "    audio_data, original_sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Total duration and chunk size\n",
    "    total_duration = len(audio_data) / original_sr\n",
    "    chunk_size = int(30 * original_sr)  # 30 seconds in samples\n",
    "\n",
    "    # Split the audio into chunks\n",
    "    chunks = [audio_data[i:i + chunk_size] for i in range(0, len(audio_data), chunk_size)]\n",
    "    valid_chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "\n",
    "    # Metadata\n",
    "    metadata_rows.append({\"File ID\": file_id,\n",
    "                          \"Total Duration (s)\": total_duration,\n",
    "                          \"Total Chunks\": len(chunks),\n",
    "                          \"Valid Chunks\": len(valid_chunks),\n",
    "                          \"Story Type\": story_type,\n",
    "                          \"Original Sample Rate\": original_sr})\n",
    "\n",
    "    # Valid Chunks\n",
    "    for i, chunk in enumerate(valid_chunks):\n",
    "        chunk_rows.append({\"File ID\": file_id,\n",
    "                           \"Chunk ID\": f\"{file_id}_chunk{i + 1}\",\n",
    "                           \"Chunk Data\": chunk,\n",
    "                           \"Story Type\": story_type,\n",
    "                           \"Sample Rate\": original_sr})\n",
    "\n",
    "# Convert to DataFrames\n",
    "metadata_df = pd.DataFrame(metadata_rows)\n",
    "chunks_df = pd.DataFrame(chunk_rows)\n",
    "\n",
    "# Display Results\n",
    "print(\"Summary of Audio Files:\")\n",
    "display(metadata_df)\n",
    "print(\"\\nSummary of Valid Audio Chunks:\")\n",
    "display(chunks_df)\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total Files Processed: {len(metadata_df)}\")\n",
    "print(f\"Total Chunks Created: {metadata_df['Total Chunks'].sum()}\")\n",
    "print(f\"Total Valid Chunks (30s): {metadata_df['Valid Chunks'].sum()}\")\n",
    "print(f\"Unique Sample Rates: {metadata_df['Original Sample Rate'].unique()}\")\n",
    "\n",
    "# True and Deceptive Story Distribution\n",
    "valid_chunk_labels = chunks_df['Story Type'].value_counts()\n",
    "print(\"\\nCount of True and Deceptive Stories from Valid Chunks (30s):\")\n",
    "for story_type, count in valid_chunk_labels.items():\n",
    "    print(f\"{story_type}: {count} chunks\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 Dataset**\n",
    "\n",
    "The datasets for this project are based on the MLEnd Deception Dataset, which consists of 100 audio recordings, each labeled as either `true_story` or `deceptive_story`. These datasets are created to train, validate, and evaluate machine learning models for deception detection. Below, we describe the datasets, the process of building them, the methods used to ensure independence and validity, and any limitations observed.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.1 Dataset Creation**\n",
    "\n",
    "### **5.1.1 Preprocessing and Chunking**\n",
    "- **Original Data**:\n",
    "  - The MLEnd Deception Dataset consists of 100 audio files, each representing a narrated story labeled as `true_story` (0) or `deceptive_story` (1).\n",
    "\n",
    "- **Chunking**:\n",
    "  - Each audio file was split into 30-second chunks. Chunks shorter than 30 seconds were discarded as non-valid chunks to maintain consistency and standardization.\n",
    "  - Valid chunks were assigned a unique identifier, with the format `FileID_ChunkID` (e.g., `00001.wav_chunk1`).\n",
    "  - All chunks from the same audio file retained the same `FileID`, ensuring the ability to track which chunks originated from the same source.\n",
    "\n",
    "- **Chunk Statistics**:\n",
    "  - A total of 520 chunks were created, of which 420 were valid (≥30 seconds) and 100 were discarded as non-valid.\n",
    "\n",
    "- **Metadata Recording**:\n",
    "  - For each valid chunk, the following metadata was recorded:\n",
    "    - `FileID`: Original file identifier.\n",
    "    - `ChunkID`: Unique identifier for the chunk.\n",
    "    - `Duration`: Length of the audio chunk.\n",
    "    - `Label`: Binary label (`true_story` or `deceptive_story`).\n",
    "\n",
    "---\n",
    "\n",
    "### **5.1.2 Feature Extraction**\n",
    "For each valid chunk, the following four audio features were extracted:\n",
    "- **Power**: Measures the energy of the audio signal.\n",
    "- **Pitch Mean**: Represents the average pitch of the audio.\n",
    "- **Pitch Standard Deviation**: Captures variations in pitch.\n",
    "- **Voiced Fraction**: Proportion of time the audio contains voiced sounds.\n",
    "\n",
    "These features were combined into a feature matrix, where each row corresponds to a chunk and the columns represent the extracted features.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.2 Training and Validation Dataset**\n",
    "\n",
    "### **5.2.1 Data Splitting**\n",
    "- **Purpose**:\n",
    "  - The dataset was split into training (70%) and validation (30%) sets to build and evaluate models.\n",
    "  - Ensuring independence by preventing data leakage (i.e., chunks from the same file do not appear in both sets).\n",
    "\n",
    "- **Method**:\n",
    "  - Used `StratifiedGroupKFold` to split the data while:\n",
    "    - Maintaining balanced class distributions in both sets.\n",
    "    - Grouping chunks by `FileID` to ensure all chunks from the same file were placed in either the training or validation set.\n",
    "\n",
    "- **Result**:\n",
    "  - Training Set:\n",
    "    - Contains 276 valid chunks.\n",
    "    - Balanced class distribution: ~48% `true_story`, ~52% `deceptive_story`.\n",
    "  - Validation Set:\n",
    "    - Contains 144 valid chunks.\n",
    "    - Balanced class distribution: ~48% `true_story`, ~52% `deceptive_story`.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2.2 Visualization**\n",
    "- **Exploring Class Distribution**:\n",
    "  - Visualized the class distributions in both training and validation sets to confirm balance.\n",
    "- **Feature Analysis**:\n",
    "  - Explored the distribution of each feature (e.g., Power, Pitch Mean, etc.) using histograms and box plots to identify patterns and outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.3 Limitations**\n",
    "1. **Dataset Size**:\n",
    "   - The dataset is relatively small, with only 420 valid chunks, which may limit the generalizability of the models.\n",
    "\n",
    "2. **Class Imbalance**:\n",
    "   - While the class distribution is approximately balanced, slight variations may still affect the model's sensitivity to one class over the other.\n",
    "\n",
    "3. **Language and Accent Variability**:\n",
    "   - Differences in language, accents, and speaking styles across the dataset may introduce variability that the features cannot fully capture.\n",
    "\n",
    "4. **Limited Features**:\n",
    "   - Only four audio features are used, which may not capture all the nuances needed for deception detection. Future work could explore more sophisticated feature extraction techniques.\n",
    "\n",
    "5. **Chunk Independence**:\n",
    "   - While grouping chunks by `FileID` ensures no data leakage, chunks within the same file may still exhibit similarities that could bias the model if overrepresented in either set.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.4 Future Improvements**\n",
    "- **Augmenting Data**:\n",
    "  - Apply data augmentation techniques to increase the diversity and size of the dataset.\n",
    "- **Additional Features**:\n",
    "  - Explore more advanced audio features or deep learning-based embeddings for richer representations.\n",
    "- **Cross-Validation**:\n",
    "  - Employ cross-validation techniques to better evaluate model performance and reduce the impact of dataset variability.\n",
    "\n",
    "This methodology ensures the creation of independent, balanced, and meaningful datasets for building and evaluating the machine learning models while acknowledging limitations and areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and results\n",
    "\n",
    "Carry out your experiments here. Analyse and explain your results. Unexplained results are worthless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6 Experiments and Results**\n",
    "\n",
    "This section documents the experiments conducted to evaluate the performance of various machine learning models for audio-based deception detection. The results are analyzed and explained in detail, highlighting key observations and potential areas for improvement.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.1 Experiment Setup**\n",
    "\n",
    "### **6.1.1 Models Tested**\n",
    "The following models were trained and evaluated:\n",
    "1. **Logistic Regression**: A baseline linear classifier.\n",
    "2. **Random Forest**: An ensemble-based classifier capable of capturing non-linear relationships.\n",
    "3. **Gradient Boosting**: A boosting method that iteratively refines weak learners.\n",
    "4. **Support Vector Machine (SVM)**: A robust classifier for smaller datasets and high-dimensional spaces.\n",
    "\n",
    "### **6.1.2 Evaluation Metrics**\n",
    "- **Accuracy**: Measures the proportion of correct predictions.\n",
    "- **F1 Score**: Balances precision and recall, particularly important for handling class imbalance.\n",
    "\n",
    "### **6.1.3 Training and Validation Setup**\n",
    "- **Data Split**: The dataset was divided into 70% training and 30% validation using `StratifiedGroupKFold`.\n",
    "- **Feature Standardization**: All features were standardized using `StandardScaler` for consistent scaling.\n",
    "- **Validation Task**: Validation sets were used to evaluate model generalization on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.2 Experiment Results**\n",
    "\n",
    "### **6.2.1 Individual Model Performance**\n",
    "| Model                 | Training Accuracy | Validation Accuracy | Training F1 Score | Validation F1 Score |\n",
    "|-----------------------|-------------------|---------------------|-------------------|---------------------|\n",
    "| Logistic Regression   | 56.88%           | 49.31%             | 49.79%           | 27.72%             |\n",
    "| Random Forest         | 79.71%           | 53.47%             | 79.41%           | 43.70%             |\n",
    "| Gradient Boosting     | 89.86%           | 53.47%             | 89.63%           | 46.40%             |\n",
    "| SVM                   | 79.35%           | 50.00%             | 79.27%           | 44.62%             |\n",
    "\n",
    "### **6.2.2 Ensemble Performance**\n",
    "An ensemble model combining **Gradient Boosting** and **SVM** was evaluated using soft voting:\n",
    "- **Validation Accuracy**: 52.08%\n",
    "- **Validation F1 Score**: 43.90%\n",
    "\n",
    "---\n",
    "\n",
    "## **6.3 Analysis of Results**\n",
    "\n",
    "### **6.3.1 Observations**\n",
    "1. **Gradient Boosting Performance**:\n",
    "   - Achieved the highest training accuracy and F1 score, indicating strong learning on the training set.\n",
    "   - However, its validation scores suggest potential overfitting, as it struggles to generalize to unseen data.\n",
    "\n",
    "2. **Random Forest Performance**:\n",
    "   - Exhibited a good balance between training and validation scores, suggesting better generalization than Gradient Boosting.\n",
    "\n",
    "3. **SVM Performance**:\n",
    "   - Performed consistently across training and validation, highlighting its robustness for smaller datasets.\n",
    "\n",
    "4. **Logistic Regression**:\n",
    "   - Had the lowest performance among all models, indicating that the data's decision boundary is non-linear and cannot be effectively captured by a linear model.\n",
    "\n",
    "5. **Ensemble Model**:\n",
    "   - Combined the strengths of Gradient Boosting and SVM, leading to slightly improved validation F1 scores compared to individual models.\n",
    "\n",
    "---\n",
    "\n",
    "### **6.3.2 Key Insights**\n",
    "- **Feature Limitations**:\n",
    "  - The four extracted features (Power, Pitch Mean, Pitch Standard Deviation, and Voiced Fraction) may not fully capture the complexity of deception in audio data, limiting the models' performance.\n",
    "  \n",
    "- **Overfitting in Complex Models**:\n",
    "  - Gradient Boosting's strong performance on training data but weaker generalization on validation data indicates overfitting. More regularization or additional training data may help mitigate this.\n",
    "\n",
    "- **Class Balance**:\n",
    "  - Balanced class distribution in training and validation sets contributed to consistent performance across metrics but may not fully address nuances in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.4 Limitations**\n",
    "1. **Small Dataset**:\n",
    "   - The relatively small size of the dataset (420 valid chunks) limits the models' ability to generalize, particularly for complex classifiers.\n",
    "\n",
    "2. **Feature Representation**:\n",
    "   - Using only four audio features may not adequately represent the intricacies of deception, leading to limited predictive power.\n",
    "\n",
    "3. **Chunk Dependency**:\n",
    "   - Although chunks from the same file were placed in either training or validation sets, their shared characteristics could still introduce subtle dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.5 Future Improvements**\n",
    "1. **Feature Engineering**:\n",
    "   - Extract additional features, such as MFCCs, spectral features, or embeddings from pre-trained audio models, to enrich the predictor space.\n",
    "\n",
    "2. **Data Augmentation**:\n",
    "   - Introduce synthetic audio variations (e.g., pitch shifts, time stretching) to increase dataset diversity and size.\n",
    "\n",
    "3. **Advanced Models**:\n",
    "   - Experiment with deep learning approaches, such as recurrent neural networks (RNNs) or transformers, to better capture temporal and contextual information in audio data.\n",
    "\n",
    "4. **Cross-Validation**:\n",
    "   - Use cross-validation to better evaluate model performance and minimize the impact of data splits on results.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.6 Conclusion**\n",
    "The experiments highlight the potential of machine learning for deception detection using audio data. While the ensemble model showed modest improvements, the results underline the need for richer features, larger datasets, and advanced modeling techniques to achieve robust performance. These insights will guide future iterations of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSrJCR_cekPO"
   },
   "source": [
    "# 7 Conclusions\n",
    "\n",
    "Your conclusions, suggestions for improvements, etc should go here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7 Conclusions**\n",
    "\n",
    "This project explored the use of machine learning models for audio-based deception detection. By leveraging the MLEnd Deception Dataset, audio features were extracted, processed, and used to train multiple classification models. While the results demonstrate the potential of machine learning for this challenging task, they also highlight key areas for improvement. Below are the main conclusions and suggestions for future work:\n",
    "\n",
    "---\n",
    "\n",
    "## **7.1 Conclusions**\n",
    "1. **Model Performance**:\n",
    "   - Gradient Boosting and Random Forest showed the best performance among the tested models, with Gradient Boosting achieving the highest F1 score on the validation set.\n",
    "   - Logistic Regression underperformed, indicating that the dataset likely requires non-linear decision boundaries for effective classification.\n",
    "   - The ensemble model, combining Gradient Boosting and SVM, slightly improved validation F1 scores but still faced generalization challenges.\n",
    "\n",
    "2. **Feature Representation**:\n",
    "   - The four extracted features (Power, Pitch Mean, Pitch Standard Deviation, and Voiced Fraction) provided a good starting point for analysis. However, these features alone may not capture the full complexity of deception in audio data.\n",
    "\n",
    "3. **Dataset Challenges**:\n",
    "   - The small size of the dataset (420 valid chunks) and inherent variability in audio (e.g., accents, speaking styles) made it difficult for models to generalize effectively.\n",
    "   - Balancing the dataset and preventing data leakage through `StratifiedGroupKFold` ensured fairness and validity of the results.\n",
    "\n",
    "4. **Practicality of the Approach**:\n",
    "   - This non-invasive, audio-based deception detection method offers advantages over traditional polygraph tests, such as accessibility, portability, and ease of use.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.2 Suggestions for Improvements**\n",
    "1. **Feature Engineering**:\n",
    "   - Incorporate more advanced features, such as:\n",
    "     - **Mel-Frequency Cepstral Coefficients (MFCCs)** for richer frequency representation.\n",
    "     - **Spectral Features** to capture detailed audio dynamics.\n",
    "     - **Embeddings** from pre-trained audio models (e.g., OpenL3, Wav2Vec) for deeper contextual understanding.\n",
    "   - Explore temporal features or sequential patterns in the audio data using time-series analysis.\n",
    "\n",
    "2. **Data Augmentation**:\n",
    "   - Apply augmentation techniques, such as pitch shifts, time stretching, and noise injection, to increase dataset size and diversity.\n",
    "\n",
    "3. **Advanced Modeling**:\n",
    "   - Experiment with deep learning models like:\n",
    "     - **Recurrent Neural Networks (RNNs)** to capture sequential dependencies.\n",
    "     - **Convolutional Neural Networks (CNNs)** to analyze spectrograms or feature maps.\n",
    "     - **Transformers** for contextual and temporal learning on audio sequences.\n",
    "\n",
    "4. **Cross-Validation**:\n",
    "   - Implement cross-validation across multiple folds to provide a more robust evaluation of model performance.\n",
    "\n",
    "5. **Larger and More Diverse Datasets**:\n",
    "   - Expand the dataset to include more recordings with a broader range of languages, accents, and speaking styles.\n",
    "   - Incorporate external datasets, if available, to increase training data volume.\n",
    "\n",
    "6. **Interpretability and Explainability**:\n",
    "   - Investigate which features contribute most to model predictions using techniques like SHAP (SHapley Additive exPlanations) or feature importance analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.3 Final Thoughts**\n",
    "This project demonstrates the feasibility of using machine learning for audio-based deception detection, highlighting both opportunities and challenges. While initial results are promising, achieving robust and generalizable models will require significant improvements in dataset quality, feature engineering, and modeling techniques. By addressing these challenges, future iterations of this work could pave the way for practical and reliable tools for detecting deception in various real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 References\n",
    "\n",
    "Acknowledge others here (books, papers, repositories, libraries, tools) \n",
    "\n",
    "1. An Acoustic Automated Lie Detector _Alice Xue_2019.pdf\n",
    "2. A_Comprehensive_Review_on_Features_Extraction_and_Features_Matching_Techniques_for_Deception_Detection.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding whether to keep or discard audio chunks shorter than 30 seconds depends on your project goals, data characteristics, and modeling approach. Here's a detailed look at the advantages and limitations of both approaches:\n",
    "\n",
    "1. Discarding Chunks Shorter Than 30s\n",
    "Advantages:\n",
    "Consistency:\n",
    "\n",
    "All chunks are of the same duration (30s), which simplifies feature extraction and model training.\n",
    "Models trained on consistent chunk sizes avoid handling varying feature lengths (e.g., MFCC arrays).\n",
    "Avoids Data Imbalance:\n",
    "\n",
    "Shorter chunks may have different statistical properties compared to 30s chunks, potentially introducing bias in model training.\n",
    "Simplified Processing:\n",
    "\n",
    "Uniform chunk length makes batch processing easier and faster during training and inference.\n",
    "Feature Reliability:\n",
    "\n",
    "Features like MFCCs, pitch, and RMS are more stable and meaningful for consistent input durations.\n",
    "Limitations:\n",
    "Data Loss:\n",
    "\n",
    "Discarding shorter chunks reduces the size of your dataset, which can be critical if you already have a small dataset.\n",
    "Important patterns or information in those shorter chunks are lost.\n",
    "Potential Bias:\n",
    "\n",
    "If certain types of audio (e.g., deceptive stories) are more likely to have shorter durations, discarding chunks can skew the dataset.\n",
    "2. Keeping Chunks Shorter Than 30s\n",
    "Advantages:\n",
    "Maximizes Data:\n",
    "\n",
    "Retains every available audio chunk, which can be critical for small datasets.\n",
    "Helps increase the training sample size and improve model generalization.\n",
    "Preserves Information:\n",
    "\n",
    "Retains all available information, especially if shorter chunks contain important features or patterns.\n",
    "Limitations:\n",
    "Feature Variability:\n",
    "\n",
    "Shorter chunks will have fewer data points, resulting in different feature lengths (e.g., fewer MFCC frames), which may require additional preprocessing (e.g., padding or truncation).\n",
    "Impact on Features:\n",
    "\n",
    "Features like Zero-Crossing Rate (ZCR) and Energy (RMS) might be less meaningful for very short chunks.\n",
    "Padding shorter chunks with zeros can distort features like ZCR.\n",
    "Complexity:\n",
    "\n",
    "Models may need additional handling for variable input lengths, such as:\n",
    "Padding with zeros.\n",
    "Using dynamic architectures like recurrent neural networks (RNNs) or transformers.\n",
    "Possible Strategies\n",
    "1. If Consistency is Key (Discard Shorter Chunks)\n",
    "Remove chunks shorter than 30 seconds for a consistent input size.\n",
    "Works well for simpler models like logistic regression, SVMs, or decision trees.\n",
    "2. If Data is Limited (Keep Shorter Chunks)\n",
    "Retain all chunks but:\n",
    "Pad with Zeros: Extend shorter chunks to 30s by padding with zeros.\n",
    "Truncate to Consistent Size: Extract the first few seconds of shorter chunks (e.g., first 5-10 seconds) for consistency.\n",
    "Helps in small datasets where maximizing data size is critical.\n",
    "Recommendation\n",
    "If you have sufficient data (100 audio files, each with 30s chunks):\n",
    "\n",
    "Discard shorter chunks for consistency, as this simplifies the processing pipeline and ensures robust feature extraction.\n",
    "If the dataset is small or imbalanced:\n",
    "\n",
    "Keep shorter chunks to maximize data but preprocess (e.g., padding or truncating) to maintain consistency."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
