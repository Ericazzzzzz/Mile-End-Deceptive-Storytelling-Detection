{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100 audio files in the dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Story_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001.wav</th>\n",
       "      <td>Hindi</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00002.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00003.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00004.wav</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00005.wav</th>\n",
       "      <td>English</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Language       Story_type\n",
       "filename                           \n",
       "00001.wav    Hindi  deceptive_story\n",
       "00002.wav  English       true_story\n",
       "00003.wav  English  deceptive_story\n",
       "00004.wav  Bengali  deceptive_story\n",
       "00005.wav  English  deceptive_story"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages narrated in the dataset are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Language</th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Chinese, Mandarin</th>\n",
       "      <th>Marathi</th>\n",
       "      <th>Bengali</th>\n",
       "      <th>Kannada</th>\n",
       "      <th>French</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Swahilli</th>\n",
       "      <th>Telugu</th>\n",
       "      <th>Korean</th>\n",
       "      <th>Cantonese</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Language  English  Hindi  Arabic  Chinese, Mandarin  Marathi  Bengali  \\\n",
       "count          78      4       3                  2        2        1   \n",
       "\n",
       "Language  Kannada  French  Russian  Portuguese  Spanish  Swahilli  Telugu  \\\n",
       "count           1       1        1           1        1         1       1   \n",
       "\n",
       "Language  Korean  Cantonese  Italian  Sum  \n",
       "count          1          1        1  100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story_type\n",
      "deceptive_story    50\n",
      "true_story         50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data Loading \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = './MLEnd/deception/MLEndDD_stories_small/'\n",
    "MLEND_df = pd.read_csv('./MLEnd/deception/MLEndDD_story_attributes_small.csv').set_index('filename')\n",
    "\n",
    "files = [base_path + file for file in MLEND_df.index]\n",
    "\n",
    "print(f\"We have {len(files)} audio files in the dataset.\")\n",
    "display(MLEND_df.head())\n",
    "\n",
    "#Langauge and Data Distribution\n",
    "language_counts = MLEND_df['Language'].value_counts()\n",
    "language_df = pd.DataFrame(language_counts).transpose()\n",
    "language_df['Sum'] = language_counts.sum()\n",
    "\n",
    "story_type_counts = MLEND_df['Story_type'].value_counts()\n",
    "\n",
    "print(\"Languages narrated in the dataset are:\")\n",
    "display(language_df)\n",
    "print(story_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def split_audio(file_id, file_path, label, chunk_duration=30, sr=None):\n",
    "    \"\"\"\n",
    "    Splits an audio file into fixed-length chunks, analyzes valid and non-valid chunks,\n",
    "    and calculates data loss during splitting.\n",
    "\n",
    "    Args:\n",
    "        file_id (str): The name of the audio file (e.g., '00001.wav').\n",
    "        file_path (str): Path to the audio file.\n",
    "        label (str): The label for the audio file (e.g., 'true_story' or 'deceptive_story').\n",
    "        chunk_duration (int): Duration of each chunk in seconds (default is 30).\n",
    "        sr (int or None): Sampling rate. If None, the original rate is used.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metadata about the file, including total duration, valid and non-valid chunk counts, and data loss.\n",
    "        list: Information about each valid chunk, including its ID and label.\n",
    "    \"\"\"\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=sr)  # sr=None uses original sample rate\n",
    "    \n",
    "    total_duration = len(audio_data) / sample_rate #in seconds\n",
    "    chunk_size = int(chunk_duration * sample_rate)\n",
    "\n",
    "    # Split the audio into chunks of `chunk_size`\n",
    "    chunks = [audio_data[i:i + chunk_size] for i in range(0, len(audio_data), chunk_size)]\n",
    "    \n",
    "    valid_chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "    non_valid_chunks = [chunk for chunk in chunks if len(chunk) < chunk_size]\n",
    "\n",
    "    metadata = {\n",
    "        \"File ID\": file_id,\n",
    "        \"Duration (s)\": total_duration,\n",
    "        \"Sample Rate\": sample_rate,\n",
    "        \"Total Chunks\": len(chunks),\n",
    "        \"Valid Chunks (30s)\": len(valid_chunks),\n",
    "        \"Non-Valid Chunks (<30s)\": len(non_valid_chunks),\n",
    "        \"Label\": label\n",
    "    }\n",
    "\n",
    "    chunk_info = [\n",
    "        {\"File ID\": file_id, \"Chunk ID\": f\"{file_id}_chunk{i + 1}\", \"Chunk Data\": chunk, \"Label\": label, \"Sample Rate\": sample_rate}\n",
    "        for i, chunk in enumerate(valid_chunks)\n",
    "    ]\n",
    "\n",
    "    return metadata, chunk_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Audio Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Duration (s)</th>\n",
       "      <th>Sample Rate</th>\n",
       "      <th>Total Chunks</th>\n",
       "      <th>Valid Chunks (30s)</th>\n",
       "      <th>Non-Valid Chunks (&lt;30s)</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>122.167256</td>\n",
       "      <td>44100</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>125.192018</td>\n",
       "      <td>44100</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.wav</td>\n",
       "      <td>162.984127</td>\n",
       "      <td>44100</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004.wav</td>\n",
       "      <td>121.681270</td>\n",
       "      <td>44100</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00005.wav</td>\n",
       "      <td>134.189751</td>\n",
       "      <td>44100</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File ID  Duration (s)  Sample Rate  Total Chunks  Valid Chunks (30s)  \\\n",
       "0  00001.wav    122.167256        44100             5                   4   \n",
       "1  00002.wav    125.192018        44100             5                   4   \n",
       "2  00003.wav    162.984127        44100             6                   5   \n",
       "3  00004.wav    121.681270        44100             5                   4   \n",
       "4  00005.wav    134.189751        44100             5                   4   \n",
       "\n",
       "   Non-Valid Chunks (<30s)            Label  \n",
       "0                        1  deceptive_story  \n",
       "1                        1       true_story  \n",
       "2                        1  deceptive_story  \n",
       "3                        1  deceptive_story  \n",
       "4                        1  deceptive_story  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Valid Audio Chunks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk1</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk2</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk3</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00001.wav</td>\n",
       "      <td>00001.wav_chunk4</td>\n",
       "      <td>deceptive_story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00002.wav</td>\n",
       "      <td>00002.wav_chunk1</td>\n",
       "      <td>true_story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     File ID          Chunk ID            Label\n",
       "0  00001.wav  00001.wav_chunk1  deceptive_story\n",
       "1  00001.wav  00001.wav_chunk2  deceptive_story\n",
       "2  00001.wav  00001.wav_chunk3  deceptive_story\n",
       "3  00001.wav  00001.wav_chunk4  deceptive_story\n",
       "4  00002.wav  00002.wav_chunk1       true_story"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "Total Files Processed: 100\n",
      "Total Chunks Created: 520\n",
      "Total Valid Chunks(30s): 420\n",
      "Total Non-Valid Chunks (<30s): 100\n",
      "\n",
      "Count of True and Deceptive Stories from Valid Chunks (30s):\n",
      "true_story: 219 chunks\n",
      "deceptive_story: 201 chunks\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Split audio into 30s\n",
    "file_metadata = []\n",
    "audio_chunks = []\n",
    "\n",
    "for file_id in tqdm(MLEND_df.index):  \n",
    "    file_path = base_path + file_id   \n",
    "    label = MLEND_df.loc[file_id, 'Story_type'] \n",
    "\n",
    "    metadata, chunks = split_audio(file_id, file_path, label)\n",
    "    file_metadata.append(metadata) \n",
    "    audio_chunks.extend(chunks)     \n",
    "\n",
    "# Metadata DF\n",
    "metadata_df = pd.DataFrame(file_metadata)\n",
    "print(\"Summary of Audio Files:\")\n",
    "display(metadata_df.head())\n",
    "\n",
    "# Chunk Info DF\n",
    "chunks_df = pd.DataFrame(audio_chunks)\n",
    "print(\"Summary of Valid Audio Chunks:\")\n",
    "display(chunks_df[[\"File ID\", \"Chunk ID\", \"Label\"]].head())\n",
    "\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total Files Processed: {len(metadata_df)}\")\n",
    "print(f\"Total Chunks Created: {metadata_df['Total Chunks'].sum()}\")\n",
    "print(f\"Total Valid Chunks(30s): {metadata_df['Valid Chunks (30s)'].sum()}\")\n",
    "print(f\"Total Non-Valid Chunks (<30s): {metadata_df['Non-Valid Chunks (<30s)'].sum()}\")\n",
    "\n",
    "# True and Deceptive Distribution after splitting\n",
    "valid_chunk_labels = chunks_df['Label'].value_counts()\n",
    "print(\"\\nCount of True and Deceptive Stories from Valid Chunks (30s):\")\n",
    "for label, count in valid_chunk_labels.items():\n",
    "    print(f\"{label}: {count} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "\n",
    "A total of 100 audio recordings - 50 true and 50 deceptive \n",
    "16 languages - English is the most dominant, followed by Hindi, Arabic, Chinese (Mandarin), Marathi and other languages as shown in the code above. \n",
    "\n",
    "After splitted the audio into 30s chunks and discard those less than 30s chunks to remain consistency. As a result,we have remaning of 420 valid 30s chunks, and 219 of them are true, 201 are false. This results in the following proportions:\n",
    "\n",
    "True Stories: \n",
    "219/420 ≈52.14%\n",
    "\n",
    "Deceptive Stories: \n",
    "201/420 ≈47.86%\n",
    "\n",
    "Hence, this dataset is generally considered balanced as both classes (true and deceptive) are roughly equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_features_and_labels(chunks_df, scale_audio=True):\n",
    "    \"\"\"\n",
    "    Extract MFCC, Pitch, Energy, and ZCR features from audio chunks and associate labels.\n",
    "    Includes File IDs to enable group-based splitting and avoid data leakage.\n",
    "\n",
    "    Args:\n",
    "        chunks_df (DataFrame): DataFrame containing Chunk Data, Label, Sample Rate, and File ID.\n",
    "        scale_audio (bool): Whether to scale audio amplitude.\n",
    "\n",
    "    Returns:\n",
    "        dict: Feature matrices for MFCC, Pitch, Energy, ZCR.\n",
    "        np.ndarray: Labels for each chunk.\n",
    "        list: File IDs for grouping to avoid data leakage.\n",
    "    \"\"\"\n",
    "    # Initialize \n",
    "    file_ids, mfcc_features, pitch_features, energy_features, zcr_features, labels = [], [], [], [], [], []\n",
    "\n",
    "    for index, row in tqdm(chunks_df.iterrows(), total=len(chunks_df)):\n",
    "        audio_data, label, sr, file_id = row[\"Chunk Data\"], row[\"Label\"], row[\"Sample Rate\"], row[\"File ID\"]\n",
    "\n",
    "        if scale_audio:\n",
    "            audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "\n",
    "        # Feature 1: MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13).mean(axis=1)\n",
    "        mfcc_features.append(mfcc)\n",
    "\n",
    "        # Feature 2: Pitch\n",
    "        pitch, _, _ = librosa.pyin(audio_data, fmin=80, fmax=450, sr=sr)\n",
    "        pitch_mean = np.nanmean(pitch) if np.mean(np.isnan(pitch))<1 else 0\n",
    "        pitch_std  = np.nanstd(pitch) if np.mean(np.isnan(pitch))<1 else 0\n",
    "        pitch_features.append([pitch_mean, pitch_std])\n",
    "\n",
    "        # Feature 3: Energy\n",
    "        rms = np.mean(librosa.feature.rms(y=audio_data))\n",
    "        energy_features.append([rms])\n",
    "\n",
    "        # Feature 4: Zero-Crossing Rate (ZCR)\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio_data))\n",
    "        zcr_features.append([zcr])\n",
    "\n",
    "        labels.append(1 if label == 'deceptive_story' else 0)\n",
    "        file_ids.append(file_id)\n",
    "\n",
    "    return {\n",
    "        'MFCC': np.array(mfcc_features),\n",
    "        'Pitch': np.array(pitch_features),\n",
    "        'Energy': np.array(energy_features),\n",
    "        'ZCR': np.array(zcr_features)\n",
    "    }, np.array(labels), file_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420/420 [14:14<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features, labels, and File IDs\n",
    "features, labels, file_ids = extract_features_and_labels(chunks_df, scale_audio=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC Shape: (420, 13)\n",
      "Pitch Shape: (420, 2)\n",
      "Energy Shape: (420, 1)\n",
      "ZCR Shape: (420, 1)\n",
      "Combined Feature Matrix Shape: (420, 17)\n",
      "Labels (y) Shape: (420,)\n",
      "File IDs Length: 420\n"
     ]
    }
   ],
   "source": [
    "X_mfcc = features['MFCC']\n",
    "X_pitch = features['Pitch']\n",
    "X_energy = features['Energy']\n",
    "X_zcr = features['ZCR']\n",
    "X_combined = np.hstack((X_mfcc, X_pitch, X_energy, X_zcr))\n",
    "y = labels\n",
    "\n",
    "print(f\"MFCC Shape: {X_mfcc.shape}\")\n",
    "print(f\"Pitch Shape: {X_pitch.shape}\")\n",
    "print(f\"Energy Shape: {X_energy.shape}\")\n",
    "print(f\"ZCR Shape: {X_zcr.shape}\")\n",
    "print(f\"Combined Feature Matrix Shape: {X_combined.shape}\")\n",
    "print(f\"Labels (y) Shape: {y.shape}\")\n",
    "print(f\"File IDs Length: {len(file_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Split:\n",
      "MFCC - Train: (287, 13), Validation: (133, 13)\n",
      "Pitch - Train: (287, 2), Validation: (133, 2)\n",
      "Energy - Train: (287, 1), Validation: (133, 1)\n",
      "ZCR - Train: (287, 1), Validation: (133, 1)\n",
      "Combined - Train: (287, 17), Validation: (133, 17)\n",
      "Labels - Train: 287, Validation: 133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def group_train_valid_split_and_scale(features_dict, y, groups, valid_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform group-aware train-validation split and standardize features.\n",
    "\n",
    "    Args:\n",
    "        features_dict (dict): Dictionary of feature matrices (e.g., {\"MFCC\": X_mfcc, \"Pitch\": X_pitch}).\n",
    "        y (np.ndarray): Labels.\n",
    "        groups (list or np.ndarray): Group IDs (e.g., File IDs) for each sample.\n",
    "        valid_size (float): Proportion of the data to be used as the validation set.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Scaled train and validation feature dictionaries, train and validation labels, and indices:\n",
    "               (scaled_train_features, scaled_valid_features, y_train, y_valid, train_idx, valid_idx).\n",
    "    \"\"\"\n",
    "    # Group-aware splitting\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=valid_size, random_state=random_state)\n",
    "    train_idx, valid_idx = next(gss.split(list(features_dict.values())[0], y, groups))\n",
    "\n",
    "    # Initialize dictionaries for scaled features\n",
    "    scaled_train_features = {}\n",
    "    scaled_valid_features = {}\n",
    "\n",
    "    # Split \n",
    "    for feature_name, feature_matrix in features_dict.items():\n",
    "        X_train, X_valid = feature_matrix[train_idx], feature_matrix[valid_idx]\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        scaled_train_features[feature_name] = scaler.fit_transform(X_train)\n",
    "        scaled_valid_features[feature_name] = scaler.transform(X_valid)\n",
    "\n",
    "    # Split labels\n",
    "    y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "\n",
    "    return scaled_train_features, scaled_valid_features, y_train, y_valid, train_idx, valid_idx\n",
    "\n",
    "\n",
    "features_dict = {\"MFCC\": X_mfcc, \"Pitch\": X_pitch, \"Energy\": X_energy, \"ZCR\": X_zcr, \"Combined\": X_combined}\n",
    "\n",
    "scaled_train_features, scaled_valid_features, y_train, y_valid, train_idx, valid_idx = group_train_valid_split_and_scale(\n",
    "    features_dict, y, file_ids, valid_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Train-Validation Split:\")\n",
    "for feature_name in scaled_train_features.keys():\n",
    "    print(f\"{feature_name} - Train: {scaled_train_features[feature_name].shape}, Validation: {scaled_valid_features[feature_name].shape}\")\n",
    "print(f\"Labels - Train: {len(y_train)}, Validation: {len(y_valid)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training F1 Score</th>\n",
       "      <th>Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MFCC</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MFCC</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.982578</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.387755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MFCC</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.533835</td>\n",
       "      <td>0.995951</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MFCC</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.486957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pitch</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.567944</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pitch</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.808362</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.789272</td>\n",
       "      <td>0.585987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pitch</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.940767</td>\n",
       "      <td>0.533835</td>\n",
       "      <td>0.932271</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pitch</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.599303</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>0.397906</td>\n",
       "      <td>0.457627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Energy</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.436090</td>\n",
       "      <td>0.278788</td>\n",
       "      <td>0.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Energy</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.766551</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>0.663317</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Energy</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.864111</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.825112</td>\n",
       "      <td>0.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Energy</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.599303</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.319527</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ZCR</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ZCR</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.735192</td>\n",
       "      <td>0.353383</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.122449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ZCR</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.871080</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>0.834081</td>\n",
       "      <td>0.303571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ZCR</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.613240</td>\n",
       "      <td>0.375940</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.045977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Combined</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.630662</td>\n",
       "      <td>0.503759</td>\n",
       "      <td>0.530973</td>\n",
       "      <td>0.365385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Combined</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>0.970954</td>\n",
       "      <td>0.319149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Combined</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.996516</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.995984</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Combined</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.971193</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature               Model  Training Accuracy  Validation Accuracy  \\\n",
       "0       MFCC  LogisticRegression           0.634146             0.518797   \n",
       "1       MFCC        RandomForest           0.982578             0.548872   \n",
       "2       MFCC    GradientBoosting           0.996516             0.533835   \n",
       "3       MFCC                 SVM           0.975610             0.556391   \n",
       "4      Pitch  LogisticRegression           0.567944             0.421053   \n",
       "5      Pitch        RandomForest           0.808362             0.511278   \n",
       "6      Pitch    GradientBoosting           0.940767             0.533835   \n",
       "7      Pitch                 SVM           0.599303             0.518797   \n",
       "8     Energy  LogisticRegression           0.585366             0.436090   \n",
       "9     Energy        RandomForest           0.766551             0.413534   \n",
       "10    Energy    GradientBoosting           0.864111             0.428571   \n",
       "11    Energy                 SVM           0.599303             0.428571   \n",
       "12       ZCR  LogisticRegression           0.571429             0.421053   \n",
       "13       ZCR        RandomForest           0.735192             0.353383   \n",
       "14       ZCR    GradientBoosting           0.871080             0.413534   \n",
       "15       ZCR                 SVM           0.613240             0.375940   \n",
       "16  Combined  LogisticRegression           0.630662             0.503759   \n",
       "17  Combined        RandomForest           0.975610             0.518797   \n",
       "18  Combined    GradientBoosting           0.996516             0.458647   \n",
       "19  Combined                 SVM           0.975610             0.526316   \n",
       "\n",
       "    Training F1 Score  Validation F1 Score  \n",
       "0            0.553191             0.360000  \n",
       "1            0.979592             0.387755  \n",
       "2            0.995951             0.483333  \n",
       "3            0.971429             0.486957  \n",
       "4            0.000000             0.000000  \n",
       "5            0.789272             0.585987  \n",
       "6            0.932271             0.569444  \n",
       "7            0.397906             0.457627  \n",
       "8            0.278788             0.242424  \n",
       "9            0.663317             0.350000  \n",
       "10           0.825112             0.406250  \n",
       "11           0.319527             0.240000  \n",
       "12           0.016000             0.000000  \n",
       "13           0.600000             0.122449  \n",
       "14           0.834081             0.303571  \n",
       "15           0.212766             0.045977  \n",
       "16           0.530973             0.365385  \n",
       "17           0.970954             0.319149  \n",
       "18           0.995984             0.357143  \n",
       "19           0.971193             0.363636  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"RandomForest\": RandomForestClassifier(max_depth=5),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"SVM\": SVC(C=1, gamma='scale', probability=True)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate_model(model, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Train a model and evaluate its performance on the training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        model: A scikit-learn classifier.\n",
    "        X_train, X_valid: Scaled training and validation features.\n",
    "        y_train, y_valid: Corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training and validation accuracy and F1 scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_valid_pred = model.predict(X_valid)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Training Accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Validation Accuracy\": accuracy_score(y_valid, y_valid_pred),\n",
    "        \"Training F1 Score\": f1_score(y_train, y_train_pred),\n",
    "        \"Validation F1 Score\": f1_score(y_valid, y_valid_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_name, X_train in scaled_train_features.items():\n",
    "    # if feature_name == \"Combined\":\n",
    "    #     continue  # Skip the \"Combined\" features\n",
    "\n",
    "    X_valid = scaled_valid_features[feature_name]\n",
    "    y_train_split, y_valid_split = y_train, y_valid\n",
    "\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        metrics = train_and_evaluate_model(model, X_train, X_valid, y_train_split, y_valid_split)\n",
    "        \n",
    "        results.append({\"Feature\": feature_name,\n",
    "                         \"Model\": model_name,\n",
    "                         \"Training Accuracy\": metrics[\"Training Accuracy\"],\n",
    "                         \"Validation Accuracy\": metrics[\"Validation Accuracy\"],\n",
    "                         \"Training F1 Score\": metrics[\"Training F1 Score\"],\n",
    "                         \"Validation F1 Score\": metrics[\"Validation F1 Score\"]})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Training Results Summary:\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Results Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Training F1 Score</th>\n",
       "      <th>Validation F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MFCC</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.466165</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.348624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pitch</td>\n",
       "      <td>0.888502</td>\n",
       "      <td>0.488722</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.492537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Energy</td>\n",
       "      <td>0.770035</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.342342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZCR</td>\n",
       "      <td>0.745645</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.592179</td>\n",
       "      <td>0.106383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.991935</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  Training Accuracy  Validation Accuracy  Training F1 Score  \\\n",
       "0      MFCC           0.993031             0.466165           0.991935   \n",
       "1     Pitch           0.888502             0.488722           0.863248   \n",
       "2    Energy           0.770035             0.451128           0.645161   \n",
       "3       ZCR           0.745645             0.368421           0.592179   \n",
       "4  Combined           0.993031             0.473684           0.991935   \n",
       "\n",
       "   Validation F1 Score  \n",
       "0             0.348624  \n",
       "1             0.492537  \n",
       "2             0.342342  \n",
       "3             0.106383  \n",
       "4             0.285714  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Ensemble models\n",
    "def train_ensemble_voting(X_train, X_valid, y_train, y_valid, models_dict):\n",
    "    \"\"\"\n",
    "    Train an ensemble voting classifier and evaluate it on training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features.\n",
    "        X_valid (np.ndarray): Validation features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        y_valid (np.ndarray): Validation labels.\n",
    "        models_dict (dict): Dictionary of models to include in the ensemble.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training and validation accuracy and F1 scores.\n",
    "    \"\"\"\n",
    "    # Create a VotingClassifier with soft voting\n",
    "    ensemble = VotingClassifier(estimators=list(models_dict.items()), voting='soft')\n",
    "    ensemble.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on training and validation sets\n",
    "    y_train_pred = ensemble.predict(X_train)\n",
    "    y_valid_pred = ensemble.predict(X_valid)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"Training Accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "        \"Validation Accuracy\": accuracy_score(y_valid, y_valid_pred),\n",
    "        \"Training F1 Score\": f1_score(y_train, y_train_pred),\n",
    "        \"Validation F1 Score\": f1_score(y_valid, y_valid_pred),\n",
    "    }\n",
    "\n",
    "    return ensemble, metrics\n",
    "\n",
    "\n",
    "# Initialize results list for ensembles\n",
    "ensemble_results = []\n",
    "\n",
    "# Train and evaluate ensemble models for each feature set (excluding \"Combined\")\n",
    "for feature_name, X_train in scaled_train_features.items():\n",
    "    # if feature_name == \"Combined\":\n",
    "    #     continue  # Skip the \"Combined\" features\n",
    "\n",
    "    X_valid = scaled_valid_features[feature_name]\n",
    "    y_train_split, y_valid_split = y_train, y_valid\n",
    "\n",
    "    # Train ensemble using individual models for this feature\n",
    "    ensemble_model, metrics = train_ensemble_voting(\n",
    "        X_train, X_valid, y_train_split, y_valid_split, models\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    ensemble_results.append({\n",
    "        \"Feature\": feature_name,\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "# Convert ensemble results to a DataFrame\n",
    "ensemble_results_df = pd.DataFrame(ensemble_results)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nEnsemble Results Summary:\")\n",
    "display(ensemble_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Models for Each Feature:\n",
      "MFCC: GradientBoosting (Validation F1: 0.4918032786885246, Validation Accuracy: 0.5338345864661654)\n",
      "Pitch: GradientBoosting (Validation F1: 0.5694444444444444, Validation Accuracy: 0.5338345864661654)\n",
      "Energy: GradientBoosting (Validation F1: 0.40625, Validation Accuracy: 0.42857142857142855)\n",
      "ZCR: GradientBoosting (Validation F1: 0.30357142857142855, Validation Accuracy: 0.41353383458646614)\n",
      "Combined: LogisticRegression (Validation F1: 0.36538461538461536, Validation Accuracy: 0.5037593984962406)\n"
     ]
    }
   ],
   "source": [
    "# Group results by feature and select the model with the highest validation F1 score\n",
    "best_models = {}\n",
    "for feature_name in results_df[\"Feature\"].unique():\n",
    "    feature_group = results_df[results_df[\"Feature\"] == feature_name]\n",
    "    best_model_row = feature_group.loc[feature_group[\"Validation F1 Score\"].idxmax()]\n",
    "    \n",
    "    best_models[feature_name] = {\n",
    "        \"Model Name\": best_model_row[\"Model\"],\n",
    "        \"Validation F1 Score\": best_model_row[\"Validation F1 Score\"],\n",
    "        \"Validation Accuracy\": best_model_row[\"Validation Accuracy\"]\n",
    "    }\n",
    "\n",
    "# Display the selected models\n",
    "print(\"Best Models for Each Feature:\")\n",
    "for feature, model_info in best_models.items():\n",
    "    print(f\"{feature}: {model_info['Model Name']} (Validation F1: {model_info['Validation F1 Score']}, Validation Accuracy: {model_info['Validation Accuracy']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best models\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "best_models = {\n",
    "    \"MFCC\": GradientBoostingClassifier(),\n",
    "    \"Pitch\": RandomForestClassifier(),\n",
    "    \"Energy\": GradientBoostingClassifier(),\n",
    "    \"ZCR\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train the selected best models on their respective features\n",
    "trained_models = {}\n",
    "for feature_name, model in best_models.items():\n",
    "    X_train_feature = scaled_train_features[feature_name]\n",
    "    model.fit(X_train_feature, y_train)\n",
    "    trained_models[feature_name] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def majority_voting(trained_models, scaled_valid_features):\n",
    "    \"\"\"\n",
    "    Perform majority voting using predictions from trained models.\n",
    "    \n",
    "    Args:\n",
    "        trained_models (dict): Trained models for each feature.\n",
    "        scaled_valid_features (dict): Scaled validation features for each feature.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Final predictions using majority voting.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for feature_name, model in trained_models.items():\n",
    "        X_valid_feature = scaled_valid_features[feature_name]\n",
    "        preds = model.predict(X_valid_feature)\n",
    "        predictions.append(preds)\n",
    "    \n",
    "    # Transpose and vote\n",
    "    predictions = np.array(predictions).T\n",
    "    majority_votes = np.array([Counter(row).most_common(1)[0][0] for row in predictions])\n",
    "    return majority_votes\n",
    "\n",
    "# Get majority voting predictions on the validation set\n",
    "y_valid_pred = majority_voting(trained_models, scaled_valid_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Feature Ensemble Validation Metrics:\n",
      "Validation Accuracy: 0.4962\n",
      "Validation F1 Score: 0.4553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Calculate validation metrics\n",
    "inter_feature_metrics = {\n",
    "    \"Validation Accuracy\": accuracy_score(y_valid, y_valid_pred),\n",
    "    \"Validation F1 Score\": f1_score(y_valid, y_valid_pred)\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"Inter-Feature Ensemble Validation Metrics:\")\n",
    "print(f\"Validation Accuracy: {inter_feature_metrics['Validation Accuracy']:.4f}\")\n",
    "print(f\"Validation F1 Score: {inter_feature_metrics['Validation F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "Baseline Model on Combined Features:\n",
      " - Validation Accuracy: 0.5263\n",
      " - Validation F1 Score: 0.3636\n",
      "\n",
      "Inter-Feature Ensemble:\n",
      " - Validation Accuracy: 0.4812\n",
      " - Validation F1 Score: 0.4202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter\n",
    "\n",
    "# Train a Baseline Model on Combined Features\n",
    "def train_baseline_combined(X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Train a baseline model on combined features and calculate validation metrics.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features (combined).\n",
    "        X_valid (np.ndarray): Validation features (combined).\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        y_valid (np.ndarray): Validation labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation accuracy and F1 score for the baseline model.\n",
    "    \"\"\"\n",
    "    # Define and train the baseline model (e.g., Gradient Boosting)\n",
    "    baseline_model = SVC()\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_valid_pred = baseline_model.predict(X_valid)\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    metrics = {\n",
    "        \"Validation Accuracy\": accuracy_score(y_valid, y_valid_pred),\n",
    "        \"Validation F1 Score\": f1_score(y_valid, y_valid_pred)\n",
    "    }\n",
    "    return metrics, baseline_model\n",
    "\n",
    "\n",
    "# Inter-Feature Ensemble Function\n",
    "def inter_feature_ensemble(trained_models, scaled_valid_features, y_valid):\n",
    "    \"\"\"\n",
    "    Perform inter-feature ensemble using majority voting on validation predictions.\n",
    "    \n",
    "    Args:\n",
    "        trained_models (dict): Trained models for each feature.\n",
    "        scaled_valid_features (dict): Scaled validation features for each feature.\n",
    "        y_valid (np.ndarray): Validation labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation accuracy and F1 score for the inter-feature ensemble.\n",
    "    \"\"\"\n",
    "    # Generate predictions for each feature model\n",
    "    predictions_list = []\n",
    "    for feature_name, model in trained_models.items():\n",
    "        X_valid_feature = scaled_valid_features[feature_name]\n",
    "        predictions = model.predict(X_valid_feature)\n",
    "        predictions_list.append(predictions)\n",
    "    \n",
    "    # Perform majority voting\n",
    "    predictions = np.array(predictions_list).T\n",
    "    majority_votes = [Counter(row).most_common(1)[0][0] for row in predictions]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"Validation Accuracy\": accuracy_score(y_valid, majority_votes),\n",
    "        \"Validation F1 Score\": f1_score(y_valid, majority_votes)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Step 1: Train the Baseline Model on Combined Features\n",
    "baseline_metrics, baseline_model = train_baseline_combined(\n",
    "    X_combined_train, X_combined_valid, y_train, y_valid)\n",
    "\n",
    "# Step 2: Train and Evaluate Inter-Feature Ensemble\n",
    "# Train the best models on their respective features\n",
    "trained_models = {}\n",
    "for feature_name, model in best_models.items():\n",
    "    X_train_feature = scaled_train_features[feature_name]\n",
    "    model.fit(X_train_feature, y_train)\n",
    "    trained_models[feature_name] = model\n",
    "\n",
    "# Evaluate inter-feature ensemble\n",
    "ensemble_metrics = inter_feature_ensemble(trained_models, scaled_valid_features, y_valid)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Baseline Model on Combined Features:\")\n",
    "print(f\" - Validation Accuracy: {baseline_metrics['Validation Accuracy']:.4f}\")\n",
    "print(f\" - Validation F1 Score: {baseline_metrics['Validation F1 Score']:.4f}\")\n",
    "\n",
    "print(\"\\nInter-Feature Ensemble:\")\n",
    "print(f\" - Validation Accuracy: {ensemble_metrics['Validation Accuracy']:.4f}\")\n",
    "print(f\" - Validation F1 Score: {ensemble_metrics['Validation F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear']  # Focus only on compatible penalties and solvers\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, scoring='f1', cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
